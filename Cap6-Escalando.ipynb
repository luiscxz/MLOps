{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b039db",
   "metadata": {},
   "source": [
    "**Tabla de contenido**\n",
    "\n",
    "- [Technical requirements](#Technical-requirements)\n",
    "- [Scaling with Spark](#Scaling-with-spark)\n",
    "    - [Consejos y trucos de Spark](#Consejos-y-trucos-de-spark)\n",
    "    - [Spark on the cloud](#Spark-on-the-cloud)\n",
    "        - [Ejemplo de AWS EMR](#Ejemplo-de-AWS-EMR)\n",
    "- [Poner en marcha infraestructura sin servidor](#Poner-en-marcha-infraestructura-sin-servidor)\n",
    "- [Containerizing at scale with Kubernetes](#Containerizing-at-scale-with-Kubernetes)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1fbf95",
   "metadata": {},
   "source": [
    "El capítulo anterior trató sobre cómo iniciar la conversación sobre cómo hacemos llegar nuestras soluciones al mundo a través de diferentes patrones de implementación, así como algunas de las herramientas que podemos utilizar para hacerlo. Este capítulo tiene como objetivo ampliar esa conversación discutiendo los conceptos y herramientas que podemos usar para escalar nuestras soluciones y hacer frente a grandes volúmenes de datos o tráfico.\n",
    "\n",
    "Ejecutar algunos modelos simples de Aprendizaje Automático (ML) en unos pocos miles de puntos de datos en tu computadora portátil es un buen ejercicio, especialmente cuando estás realizando los pasos de descubrimiento y prueba de concepto que mencionamos anteriormente al comienzo de cualquier proyecto de desarrollo de ML. Sin embargo, este enfoque no es apropiado si tenemos que manejar millones y millones de puntos de datos a una frecuencia relativamente alta, o si tenemos que entrenar miles de modelos de una escala similar al mismo tiempo. Esto requiere un enfoque, mentalidad y conjunto de herramientas diferentes.\n",
    "\n",
    "En las siguientes páginas, cubriremos algunos detalles del marco más popular para distribuir computaciones de datos en uso hoy en día: Apache Spark. En particular, discutiremos algunos de los puntos clave sobre cómo funciona detrás de escena, para que, en el desarrollo, podamos tomar buenas decisiones sobre cómo usarlo, antes de pasar a discutir algunos de los enfoques de ML que podemos emplear con Spark. Esto te ayudará a construir sobre algunos de los ejemplos prácticos que ya vimos anteriormente en este libro, cuando usamos Spark para resolver nuestros problemas de ML, con una comprensión teórica más concreta y ejemplos prácticos detallados adicionales proporcionados.\n",
    "\n",
    "Después de esto, aprenderás cómo escalar tu infraestructura utilizando funciones sin servidor. Luego, introduciremos brevemente algo de la teoría detrás del uso de clústeres de Kubernetes (K8s). El primero de estos proporciona un mecanismo para hacer que modelos simples escalen muy rápidamente. El segundo te permite mantener los beneficios de la contenedorización, pero también escalar horizontalmente. Finalmente, concluiremos, como de costumbre, proporcionando un resumen de lo que hemos aprendido.\n",
    "\n",
    "En este capítulo, cubriremos los siguientes temas:\n",
    "\n",
    "- Scaling with Spark\n",
    "- Spinning up serverless infrastructure\n",
    "- Containerizing at scale with Kubernetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726fa0e",
   "metadata": {},
   "source": [
    "# Technical requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94281f2",
   "metadata": {},
   "source": [
    "Para ejecutar los ejemplos en este capítulo necesitarás que las siguientes herramientas y paquetes estén instalados:\n",
    "- Apache Spark\n",
    "- Git\n",
    "- AWS CLI v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b35f9",
   "metadata": {},
   "source": [
    "# Scaling with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397a265",
   "metadata": {},
   "source": [
    "Apache Spark surgió del trabajo de algunos brillantes investigadores en la Universidad de California, Berkeley en 2012 y desde entonces, ha revolucionado la forma en que abordamos los problemas con grandes conjuntos de datos. Antes de Spark, el paradigma dominante para los grandes datos era Hadoop MapReduce, que ahora es mucho menos popular.\n",
    "\n",
    "Spark es un marco de computación en clúster, lo que significa que funciona según el principio de que varios computadoras están conectadas de tal manera que permite que las tareas computacionales se compartan. Esto nos permite coordinar estas tareas de manera efectiva. Siempre que hablamos de ejecutar trabajos de Spark, siempre hablamos del clúster en el que estamos ejecutando. Esta es la colección de computadoras que realizan las tareas, los nodos trabajadores, y la computadora que alberga la carga de trabajo organizativa, conocida como el nodo principal.\n",
    "\n",
    "Spark está escrito en Scala, un lenguaje con un fuerte sabor funcional que se compila en Máquinas Virtuales de Java (JVM). Dado que este es un libro sobre ingeniería de ML en Python, no discutimos demasiado sobre los componentes subyacentes de Scala en Spark, excepto donde nos ayudan a usarlo en nuestro trabajo. Spark tiene varias APIs populares que permiten a los programadores desarrollar con él en una variedad de lenguajes, incluido Python. Esto da lugar a la sintaxis de PySpark que hemos estado utilizando en varios ejemplos a lo largo de este libro.\n",
    "\n",
    "Entonces, ¿cómo se organiza todo esto? Bueno, antes que nada, una de las cosas que hace que Apache Spark sea increíblemente popular es la gran cantidad de conectores, componentes y API que tiene disponibles. Por ejemplo, cuatro componentes principales:\n",
    "\n",
    "- `Spark SQL, DataFrames, and Datasets`: Este componente te permite crear programas muy escalables que manejan datos estructurados. La capacidad de escribir consultas compatibles con SQL y crear tablas de datos que aprovechan el motor subyacente de Spark a través de una de las principales APIs estructuradas de Spark (Python, Java, Scala o R) da acceso muy fácil a la mayor parte de la funcionalidad de Spark.\n",
    "\n",
    "- `Spark Structured Streaming`: Este componente permite a los ingenieros trabajar con datos en streaming que, por ejemplo, son proporcionados por una solución como Apache Kafka. El diseño es increíblemente simple y permite a los desarrolladores trabajar con datos en streaming como si fuera una tabla estructurada de Spark en crecimiento, con la misma funcionalidad de consulta y manipulación que para una tabla estándar. Esto proporciona una baja barrera de entrada para crear soluciones de streaming escalables.\n",
    "\n",
    "- `GraphX`: Esta es una biblioteca que te permite implementar el procesamiento paralelo de grafos y aplicar algoritmos estándar a datos basados en grafos (por ejemplo, algoritmos como PageRank o conteo de triángulos). El proyecto GraphFrames de Databricks hace que esta funcionalidad sea aún más fácil de usar al permitirnos trabajar con API basadas en DataFrame en Spark y todavía analizar datos de grafos.\n",
    "\n",
    "- `Spark MLlib`: Por último, pero no menos importante, tenemos el componente que es más apropiado para nosotros como ingenieros de ML: la biblioteca nativa de Spark para ML. Esta biblioteca contiene la implementación de muchos algoritmos y capacidades de ingeniería de características que ya hemos visto en este libro. Poder usar las API de DataFrame en la biblioteca la hace extremadamente fácil de usar, mientras que aún nos ofrece una ruta para crear código muy poderoso. Las posibles aceleraciones que puedes obtener para tu entrenamiento de ML al usar Spark MLlib en un clúster de Spark frente a ejecutar otra biblioteca de ML en un solo hilo pueden ser enormes. Hay otros trucos que podemos aplicar a nuestras implementaciones de ML favoritas y luego usar Spark para escalarlas; veremos esto más adelante.\n",
    "\n",
    "La arquitectura de Spark se basa en la arquitectura de controlador/ejecutor. El controlador es el programa que actúa como el punto de entrada principal para la aplicación Spark y es donde se crea el objeto SparkContext. SparkContext envía tareas a los ejecutores (que se ejecutan en sus propias JVMs) y se comunica con el administrador del clúster de una manera apropiada para el administrador dado y para el modo en que la solución se está ejecutando. Una de las principales tareas del controlador es convertir el código que escribimos en un conjunto lógico de pasos en un Grafo Acíclico Dirigido (DAG) (el mismo concepto que usamos con Apache Airflow en el Capítulo 5, Patrones y Herramientas de Despliegue), y luego convertir ese DAG en un conjunto de tareas que necesitan ser ejecutadas en los recursos computacionales disponibles.\n",
    "\n",
    "En las páginas que siguen, asumiremos que estamos ejecutando Spark con el gestor de recursos Hadoop YARN, que es una de las opciones más populares y que también es utilizada por defecto por la solución AWS Elastic Map Reduce (más sobre esto más adelante). Al ejecutar con YARN en modo clúster, el programa controlador se ejecuta en un contenedor en el clúster YARN, lo que permite a un cliente enviar trabajos o solicitudes a través del controlador y luego salir (en lugar de requerir que el cliente permanezca conectado al gestor del clúster, lo que puede suceder cuando ejecutas en el llamado modo cliente, que no discutiremos aquí).\n",
    "\n",
    "El administrador de clúster es responsable de lanzar los ejecutores a través de los recursos que están disponibles en el clúster. La arquitectura de Spark nos permite, como ingenieros de ML, construir soluciones con la misma API y sintaxis, independientemente de si estamos trabajando localmente en nuestra computadora portátil o en un clúster con miles de nodos. La conexión entre el controlador, el administrador de recursos y los ejecutores es lo que permite que esta magia suceda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50caada",
   "metadata": {},
   "source": [
    "## Consejos y trucos de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792fd26",
   "metadata": {},
   "source": [
    "En esta subsección, cubriremos algunos consejos simples pero efectivos para escribir soluciones eficientes con Spark. Nos enfocaremos en elementos clave de la sintaxis destinada a la manipulación y preparación de datos, que, como se discutió en otros lugares de este libro, siempre es el primer paso en cualquier pipeline de solución basado en ML. Comencemos:\n",
    "\n",
    "1. Primero, cubriremos lo básico de escribir buen SQL en Spark. El punto de entrada para cualquier programa de Spark es el objeto SparkSession, que necesitamos importar como una instancia en nuestra aplicación. A menudo se instancia con la variable spark:\n",
    "\n",
    "En este caso:\n",
    "\n",
    "- .appName(\"BankCSV\"): Nombre de la app (aparece en la UI de Spark)\n",
    "- .master(\"local[*]\"): Usa todos los núcleos de tu CPU en modo local\n",
    "- .config(\"spark.hadoop.fs.defaultFS\", \"file:///\"): # Trabaja con archivos locales (no HDFS)\n",
    "- .getOrCreate(): Crea o reutiliza la sesión Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d75fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BankCSV\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffc821",
   "metadata": {},
   "source": [
    "2. Luego puedes ejecutar comandos de Spark SQL contra tus datos disponibles utilizando el objeto spark y el método sql:\n",
    "\n",
    "spark.sql('''select * from data_table''')\n",
    "\n",
    "Hay una variedad de formas de hacer que los datos que necesitas estén disponibles dentro de tus programas de Spark, dependiendo de dónde existan. El siguiente ejemplo ha sido tomado de parte del código que revisamos en el Capítulo 3, De Modelo a Fábrica de Modelos, y muestra cómo extraer datos en un dataframe desde un archivo csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe836d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|       job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "| 59|    admin.| married|secondary|     no| 2343.0|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|      1|\n",
      "| 56|    admin.| married|secondary|     no|   45.0|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|      1|\n",
      "| 41|technician| married|secondary|     no| 1270.0|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|      1|\n",
      "| 55|  services| married|secondary|     no| 2476.0|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|      1|\n",
      "| 54|    admin.| married| tertiary|     no|  184.0|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|      1|\n",
      "| 42|management|  single| tertiary|     no|    0.0|    yes| yes|unknown|  5|  may|     562|       2|   -1|       0| unknown|      1|\n",
      "| 56|management| married| tertiary|     no|  830.0|    yes| yes|unknown|  6|  may|    1201|       1|   -1|       0| unknown|      1|\n",
      "| 60|   retired|divorced|secondary|     no|  545.0|    yes|  no|unknown|  6|  may|    1030|       1|   -1|       0| unknown|      1|\n",
      "| 37|technician| married|secondary|     no|    1.0|    yes|  no|unknown|  6|  may|     608|       1|   -1|       0| unknown|      1|\n",
      "| 28|  services|  single|secondary|     no| 5090.0|    yes|  no|unknown|  6|  may|    1297|       3|   -1|       0| unknown|      1|\n",
      "+---+----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "file_path = lambda file: os.path.join(os.getcwd(),'data',file)\n",
    "schema = StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"marital\", StringType(), True),\n",
    "    StructField(\"education\", StringType(), True),\n",
    "    StructField(\"default\", StringType(), True),\n",
    "    StructField(\"balance\", DoubleType(), True),\n",
    "    StructField(\"housing\", StringType(), True),\n",
    "    StructField(\"loan\", StringType(), True),\n",
    "    StructField(\"contact\", StringType(), True),\n",
    "    StructField(\"day\", StringType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"duration\", StringType(), True),\n",
    "    StructField(\"campaign\", StringType(), True),\n",
    "    StructField(\"pdays\", StringType(), True),\n",
    "    StructField(\"previous\", StringType(), True),\n",
    "    StructField(\"poutcome\", StringType(), True),\n",
    "    StructField(\"deposit\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"file://\" + file_path('bank.csv'), sep=',', header=True, schema=schema)\n",
    "# Transformar la columna 'deposit' de 'yes'/'no' a 1/0\n",
    "df = df.withColumn(\"deposit\", when(df[\"deposit\"] == \"yes\", 1).otherwise(0))\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e6401",
   "metadata": {},
   "source": [
    "Como un ejemplo concreto, construyamos una UDF que examine los datos bancarios con los que trabajamos en el Capítulo 3, De Modelo a Fábrica de Modelos, para crear una nueva columna llamada 'month_as_int' que convierta la representación actual del mes en una cadena a un número entero para su posterior procesamiento. No nos preocuparemos por las divisiones de entrenamiento/prueba ni para qué podría usarse esto; en su lugar, simplemente destacaremos cómo aplicar algo de lógica a una UDF de PySpark. Comencemos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "330fec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def month_as_int(month):\n",
    "    month_number = datetime.datetime.strptime(month, \"%b\").month\n",
    "    return month_number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee643e2",
   "metadata": {},
   "source": [
    "Si queremos aplicar nuestra función dentro de Spark SQL, entonces debemos registrar la función como una UDF. Los argumentos para la función register() son el nombre registrado de la función, el nombre de la función de Python que acabamos de escribir y el tipo de retorno. El tipo de retorno es StringType() por defecto, pero lo hemos especificado explícitamente aquí:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09fee297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.month_as_int(month)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "spark.udf.register(\"monthAsInt\", month_as_int, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c274e",
   "metadata": {},
   "source": [
    "Finalmente, ahora que hemos registrado la función, podemos aplicarla a nuestros datos. Primero, crearemos una vista temporal del conjunto de datos del banco y luego ejecutaremos una consulta Spark SQL contra él que haga referencia a nuestra UDF. Ejecutar la siguiente sintaxis con el comando show() da el resultado mostrado en la Figura 6.3, que es nuestro resultado deseado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dcccd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "|age|        job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|month_as_int|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "| 59|     admin.| married|secondary|     no| 2343.0|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|      1|           5|\n",
      "| 56|     admin.| married|secondary|     no|   45.0|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|      1|           5|\n",
      "| 41| technician| married|secondary|     no| 1270.0|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|      1|           5|\n",
      "| 55|   services| married|secondary|     no| 2476.0|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|      1|           5|\n",
      "| 54|     admin.| married| tertiary|     no|  184.0|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|      1|           5|\n",
      "| 42| management|  single| tertiary|     no|    0.0|    yes| yes|unknown|  5|  may|     562|       2|   -1|       0| unknown|      1|           5|\n",
      "| 56| management| married| tertiary|     no|  830.0|    yes| yes|unknown|  6|  may|    1201|       1|   -1|       0| unknown|      1|           5|\n",
      "| 60|    retired|divorced|secondary|     no|  545.0|    yes|  no|unknown|  6|  may|    1030|       1|   -1|       0| unknown|      1|           5|\n",
      "| 37| technician| married|secondary|     no|    1.0|    yes|  no|unknown|  6|  may|     608|       1|   -1|       0| unknown|      1|           5|\n",
      "| 28|   services|  single|secondary|     no| 5090.0|    yes|  no|unknown|  6|  may|    1297|       3|   -1|       0| unknown|      1|           5|\n",
      "| 38|     admin.|  single|secondary|     no|  100.0|    yes|  no|unknown|  7|  may|     786|       1|   -1|       0| unknown|      1|           5|\n",
      "| 30|blue-collar| married|secondary|     no|  309.0|    yes|  no|unknown|  7|  may|    1574|       2|   -1|       0| unknown|      1|           5|\n",
      "| 29| management| married| tertiary|     no|  199.0|    yes| yes|unknown|  7|  may|    1689|       4|   -1|       0| unknown|      1|           5|\n",
      "| 46|blue-collar|  single| tertiary|     no|  460.0|    yes|  no|unknown|  7|  may|    1102|       2|   -1|       0| unknown|      1|           5|\n",
      "| 31| technician|  single| tertiary|     no|  703.0|    yes|  no|unknown|  8|  may|     943|       2|   -1|       0| unknown|      1|           5|\n",
      "| 35| management|divorced| tertiary|     no| 3837.0|    yes|  no|unknown|  8|  may|    1084|       1|   -1|       0| unknown|      1|           5|\n",
      "| 32|blue-collar|  single|  primary|     no|  611.0|    yes|  no|unknown|  8|  may|     541|       3|   -1|       0| unknown|      1|           5|\n",
      "| 49|   services| married|secondary|     no|   -8.0|    yes|  no|unknown|  8|  may|    1119|       1|   -1|       0| unknown|      1|           5|\n",
      "| 41|     admin.| married|secondary|     no|   55.0|    yes|  no|unknown|  8|  may|    1120|       2|   -1|       0| unknown|      1|           5|\n",
      "| 49|     admin.|divorced|secondary|     no|  168.0|    yes| yes|unknown|  8|  may|     513|       1|   -1|       0| unknown|      1|           5|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('bank_data_view')\n",
    "spark.sql('''\n",
    "select *, monthAsInt(month) as month_as_int from bank_data_view\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755da95",
   "metadata": {},
   "source": [
    "Alternativamente, podemos crear nuestra UDF con la siguiente sintaxis y aplicar el resultado a un DataFrame de Spark. Esto nos da el mismo resultado que se muestra en la captura de pantalla anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de11ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "|age|        job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|month_as_int|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "| 59|     admin.| married|secondary|     no| 2343.0|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|      1|           5|\n",
      "| 56|     admin.| married|secondary|     no|   45.0|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|      1|           5|\n",
      "| 41| technician| married|secondary|     no| 1270.0|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|      1|           5|\n",
      "| 55|   services| married|secondary|     no| 2476.0|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|      1|           5|\n",
      "| 54|     admin.| married| tertiary|     no|  184.0|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|      1|           5|\n",
      "| 42| management|  single| tertiary|     no|    0.0|    yes| yes|unknown|  5|  may|     562|       2|   -1|       0| unknown|      1|           5|\n",
      "| 56| management| married| tertiary|     no|  830.0|    yes| yes|unknown|  6|  may|    1201|       1|   -1|       0| unknown|      1|           5|\n",
      "| 60|    retired|divorced|secondary|     no|  545.0|    yes|  no|unknown|  6|  may|    1030|       1|   -1|       0| unknown|      1|           5|\n",
      "| 37| technician| married|secondary|     no|    1.0|    yes|  no|unknown|  6|  may|     608|       1|   -1|       0| unknown|      1|           5|\n",
      "| 28|   services|  single|secondary|     no| 5090.0|    yes|  no|unknown|  6|  may|    1297|       3|   -1|       0| unknown|      1|           5|\n",
      "| 38|     admin.|  single|secondary|     no|  100.0|    yes|  no|unknown|  7|  may|     786|       1|   -1|       0| unknown|      1|           5|\n",
      "| 30|blue-collar| married|secondary|     no|  309.0|    yes|  no|unknown|  7|  may|    1574|       2|   -1|       0| unknown|      1|           5|\n",
      "| 29| management| married| tertiary|     no|  199.0|    yes| yes|unknown|  7|  may|    1689|       4|   -1|       0| unknown|      1|           5|\n",
      "| 46|blue-collar|  single| tertiary|     no|  460.0|    yes|  no|unknown|  7|  may|    1102|       2|   -1|       0| unknown|      1|           5|\n",
      "| 31| technician|  single| tertiary|     no|  703.0|    yes|  no|unknown|  8|  may|     943|       2|   -1|       0| unknown|      1|           5|\n",
      "| 35| management|divorced| tertiary|     no| 3837.0|    yes|  no|unknown|  8|  may|    1084|       1|   -1|       0| unknown|      1|           5|\n",
      "| 32|blue-collar|  single|  primary|     no|  611.0|    yes|  no|unknown|  8|  may|     541|       3|   -1|       0| unknown|      1|           5|\n",
      "| 49|   services| married|secondary|     no|   -8.0|    yes|  no|unknown|  8|  may|    1119|       1|   -1|       0| unknown|      1|           5|\n",
      "| 41|     admin.| married|secondary|     no|   55.0|    yes|  no|unknown|  8|  may|    1120|       2|   -1|       0| unknown|      1|           5|\n",
      "| 49|     admin.|divorced|secondary|     no|  168.0|    yes| yes|unknown|  8|  may|     513|       1|   -1|       0| unknown|      1|           5|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "month_as_int_udf = udf(month_as_int, StringType())\n",
    "df = spark.table(\"bank_data_view\")\n",
    "df.withColumn('month_as_int', month_as_int_udf(\"month\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19be6d",
   "metadata": {},
   "source": [
    "Finalmente, PySpark también proporciona una sintaxis de decorador útil para crear nuestro UDF. El siguiente bloque de código también ofrece los mismos resultados que la captura de pantalla anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c6ed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "|age|        job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|month_as_int|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "| 59|     admin.| married|secondary|     no| 2343.0|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|      1|           5|\n",
      "| 56|     admin.| married|secondary|     no|   45.0|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|      1|           5|\n",
      "| 41| technician| married|secondary|     no| 1270.0|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|      1|           5|\n",
      "| 55|   services| married|secondary|     no| 2476.0|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|      1|           5|\n",
      "| 54|     admin.| married| tertiary|     no|  184.0|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|      1|           5|\n",
      "| 42| management|  single| tertiary|     no|    0.0|    yes| yes|unknown|  5|  may|     562|       2|   -1|       0| unknown|      1|           5|\n",
      "| 56| management| married| tertiary|     no|  830.0|    yes| yes|unknown|  6|  may|    1201|       1|   -1|       0| unknown|      1|           5|\n",
      "| 60|    retired|divorced|secondary|     no|  545.0|    yes|  no|unknown|  6|  may|    1030|       1|   -1|       0| unknown|      1|           5|\n",
      "| 37| technician| married|secondary|     no|    1.0|    yes|  no|unknown|  6|  may|     608|       1|   -1|       0| unknown|      1|           5|\n",
      "| 28|   services|  single|secondary|     no| 5090.0|    yes|  no|unknown|  6|  may|    1297|       3|   -1|       0| unknown|      1|           5|\n",
      "| 38|     admin.|  single|secondary|     no|  100.0|    yes|  no|unknown|  7|  may|     786|       1|   -1|       0| unknown|      1|           5|\n",
      "| 30|blue-collar| married|secondary|     no|  309.0|    yes|  no|unknown|  7|  may|    1574|       2|   -1|       0| unknown|      1|           5|\n",
      "| 29| management| married| tertiary|     no|  199.0|    yes| yes|unknown|  7|  may|    1689|       4|   -1|       0| unknown|      1|           5|\n",
      "| 46|blue-collar|  single| tertiary|     no|  460.0|    yes|  no|unknown|  7|  may|    1102|       2|   -1|       0| unknown|      1|           5|\n",
      "| 31| technician|  single| tertiary|     no|  703.0|    yes|  no|unknown|  8|  may|     943|       2|   -1|       0| unknown|      1|           5|\n",
      "| 35| management|divorced| tertiary|     no| 3837.0|    yes|  no|unknown|  8|  may|    1084|       1|   -1|       0| unknown|      1|           5|\n",
      "| 32|blue-collar|  single|  primary|     no|  611.0|    yes|  no|unknown|  8|  may|     541|       3|   -1|       0| unknown|      1|           5|\n",
      "| 49|   services| married|secondary|     no|   -8.0|    yes|  no|unknown|  8|  may|    1119|       1|   -1|       0| unknown|      1|           5|\n",
      "| 41|     admin.| married|secondary|     no|   55.0|    yes|  no|unknown|  8|  may|    1120|       2|   -1|       0| unknown|      1|           5|\n",
      "| 49|     admin.|divorced|secondary|     no|  168.0|    yes| yes|unknown|  8|  may|     513|       1|   -1|       0| unknown|      1|           5|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "@udf(\"int\")\n",
    "def month_as_int_udf(month):\n",
    "    month_number = datetime.datetime.strptime(month, \"%b\").month\n",
    "    return month_number\n",
    "df.withColumn('month_as_int', month_as_int_udf(\"month\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ccdbb3",
   "metadata": {},
   "source": [
    "Esto muestra cómo podemos aplicar cierta lógica simple en un UDF, pero para desplegar un modelo a gran escala usando este enfoque, tenemos que poner la lógica de ML dentro de la función y aplicarla de la misma manera. Esto puede volverse un poco complicado si queremos trabajar con algunas de las herramientas estándar a las que estamos acostumbrados en el mundo de la ciencia de datos, como pandas y scikit-learn. Afortunadamente, hay otra opción que podemos usar que tiene algunos beneficios. La discutiremos ahora.\n",
    "\n",
    "Las UDF que se están considerando actualmente tienen un pequeño problema cuando trabajamos en Python, ya que la traducción de datos entre la JVM y Python puede tardar un tiempo. Una forma de evitar esto es utilizar lo que se conoce como UDFs de pandas, que utilizan la biblioteca Apache Arrow en segundo plano para garantizar que los datos se lean rápidamente para la ejecución de nuestras UDFs. Esto nos brinda la flexibilidad de las UDFs sin ninguna desaceleración.\n",
    "\n",
    "Las UDFs de pandas también son extremadamente poderosas porque funcionan con la sintaxis de, sí, lo adivinaste, los objetos Series y DataFrame de pandas. Esto significa que muchos científicos de datos que están acostumbrados a trabajar con pandas para construir modelos localmente pueden adaptar fácilmente su código para escalar utilizando Spark.\n",
    "\n",
    "Como ejemplo, vamos a recorrer cómo aplicar un clasificador simple al conjunto de datos de vinos que usamos anteriormente en este libro. Cabe señalar que el modelo no fue optimizado para estos datos; solo estamos mostrando un ejemplo de cómo aplicar un clasificador preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae91af97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">kernel&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;rbf&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('degree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">degree&nbsp;</td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">gamma&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;scale&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('coef0',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">coef0&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('shrinking',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">shrinking&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('probability',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">probability&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cache_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">cache_size&nbsp;</td>\n",
       "            <td class=\"value\">200</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decision_function_shape',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">decision_function_shape&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;ovr&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('break_ties',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">break_ties&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "import sklearn.datasets\n",
    "clf = sklearn.svm.SVC()\n",
    "X, y = sklearn.datasets.load_wine(return_X_y=True) \n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39983ef9",
   "metadata": {},
   "source": [
    "Luego podemos llevar los datos de las características a un DataFrame de Spark para mostrarte cómo aplicar la UDF de pandas en etapas posteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f96fc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(X.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1516d",
   "metadata": {},
   "source": [
    "Los UDFs de pandas son muy fáciles de definir. Solo tenemos que escribir nuestra lógica en una función y luego agregar el decorador @pandas_udf, donde también debemos proporcionar el tipo de salida de la función. En el caso más simple, podemos simplemente envolver el proceso (normalmente serial o solo paralelizado localmente) de realizar una predicción con el modelo entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c751efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(returnType=IntegerType())\n",
    "def predict_pd_udf(*cols):\n",
    "    X = pd.concat(cols, axis=1)\n",
    "    return pd.Series(clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc4b88b",
   "metadata": {},
   "source": [
    "Finalmente, podemos aplicar esto al DataFrame de Spark que contiene los datos pasando los valores de entrada apropiados que necesitábamos para nuestra función. En este caso, vamos a pasar los nombres de las columnas de las características, de las cuales hay 13:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a2e9492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+----+-----+----+----+----+----+----+----+----+------+-----+\n",
      "|   _1|  _2|  _3|  _4|   _5|  _6|  _7|  _8|  _9| _10| _11| _12|   _13|class|\n",
      "+-----+----+----+----+-----+----+----+----+----+----+----+----+------+-----+\n",
      "|14.23|1.71|2.43|15.6|127.0| 2.8|3.06|0.28|2.29|5.64|1.04|3.92|1065.0|    0|\n",
      "| 13.2|1.78|2.14|11.2|100.0|2.65|2.76|0.26|1.28|4.38|1.05| 3.4|1050.0|    0|\n",
      "|13.16|2.36|2.67|18.6|101.0| 2.8|3.24| 0.3|2.81|5.68|1.03|3.17|1185.0|    0|\n",
      "|14.37|1.95| 2.5|16.8|113.0|3.85|3.49|0.24|2.18| 7.8|0.86|3.45|1480.0|    0|\n",
      "|13.24|2.59|2.87|21.0|118.0| 2.8|2.69|0.39|1.82|4.32|1.04|2.93| 735.0|    2|\n",
      "| 14.2|1.76|2.45|15.2|112.0|3.27|3.39|0.34|1.97|6.75|1.05|2.85|1450.0|    0|\n",
      "|14.39|1.87|2.45|14.6| 96.0| 2.5|2.52| 0.3|1.98|5.25|1.02|3.58|1290.0|    0|\n",
      "|14.06|2.15|2.61|17.6|121.0| 2.6|2.51|0.31|1.25|5.05|1.06|3.58|1295.0|    0|\n",
      "|14.83|1.64|2.17|14.0| 97.0| 2.8|2.98|0.29|1.98| 5.2|1.08|2.85|1045.0|    0|\n",
      "|13.86|1.35|2.27|16.0| 98.0|2.98|3.15|0.22|1.85|7.22|1.01|3.55|1045.0|    0|\n",
      "| 14.1|2.16| 2.3|18.0|105.0|2.95|3.32|0.22|2.38|5.75|1.25|3.17|1510.0|    0|\n",
      "|14.12|1.48|2.32|16.8| 95.0| 2.2|2.43|0.26|1.57| 5.0|1.17|2.82|1280.0|    0|\n",
      "|13.75|1.73|2.41|16.0| 89.0| 2.6|2.76|0.29|1.81| 5.6|1.15| 2.9|1320.0|    0|\n",
      "|14.75|1.73|2.39|11.4| 91.0| 3.1|3.69|0.43|2.81| 5.4|1.25|2.73|1150.0|    0|\n",
      "|14.38|1.87|2.38|12.0|102.0| 3.3|3.64|0.29|2.96| 7.5| 1.2| 3.0|1547.0|    0|\n",
      "|13.63|1.81| 2.7|17.2|112.0|2.85|2.91| 0.3|1.46| 7.3|1.28|2.88|1310.0|    0|\n",
      "| 14.3|1.92|2.72|20.0|120.0| 2.8|3.14|0.33|1.97| 6.2|1.07|2.65|1280.0|    0|\n",
      "|13.83|1.57|2.62|20.0|115.0|2.95| 3.4| 0.4|1.72| 6.6|1.13|2.57|1130.0|    0|\n",
      "|14.19|1.59|2.48|16.5|108.0| 3.3|3.93|0.32|1.86| 8.7|1.23|2.82|1680.0|    0|\n",
      "|13.64| 3.1|2.56|15.2|116.0| 2.7|3.03|0.17|1.66| 5.1|0.96|3.36| 845.0|    0|\n",
      "+-----+----+----+----+-----+----+----+----+----+----+----+----+------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_names = ['_{}'.format(x) for x in range(1, 14)]\n",
    "df_pred = df.select('*', predict_pd_udf(*col_names).alias('class'))\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4e9f6",
   "metadata": {},
   "source": [
    "Y eso completa nuestro recorrido exprés por los UDFs y los pandas UDFs en Spark, los cuales nos permiten tomar la lógica serial de Python, como transformaciones de datos o nuestros modelos de ML, y aplicarla de manera marcadamente paralela. En la siguiente sección, nos centraremos en cómo prepararnos para realizar cálculos basados en Spark en la nube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd09953",
   "metadata": {},
   "source": [
    "## Spark on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b82bbf",
   "metadata": {},
   "source": [
    "Como debería estar claro a partir de la discusión anterior, escribir y desplegar soluciones de ML basadas en PySpark se puede hacer en tu portátil, pero para que puedas ver los beneficios al trabajar a gran escala, debes contar con un clúster de computación de tamaño adecuado. Proporcionar este tipo de infraestructura puede ser un proceso largo y doloroso, pero como ya se ha discutido en este libro, existe una gran cantidad de opciones de infraestructura disponibles por parte de los principales proveedores de nube pública.\n",
    "\n",
    "Para Spark, AWS tiene una solución particularmente buena llamada Elastic Map Reduce (EMR), que es una plataforma de big data gestionada que te permite configurar fácilmente clústeres de varios tipos dentro del ecosistema de big data. En este libro, nos centraremos en soluciones basadas en Spark, por lo que nos enfocaremos en crear y usar clústeres que tengan herramientas de Spark disponibles.\n",
    "\n",
    "En la siguiente sección, revisaremos un ejemplo concreto de cómo levantar un clúster de Spark en EMR y luego desplegar una aplicación sencilla basada en Spark MLlib en él. Así que, con eso, ¡exploremos Spark en la nube con AWS EMR!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78de70e",
   "metadata": {},
   "source": [
    "### Ejemplo de AWS EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee532dc8",
   "metadata": {},
   "source": [
    "Para entender cómo funciona EMR, continuaremos en la línea práctica que seguirá el resto de este libro y nos sumergiremos en un ejemplo. Comenzaremos aprendiendo cómo crear un clúster completamente nuevo antes de discutir cómo escribir y desplegar nuestra primera solución de PySpark ML en él. ¡Comencemos!\n",
    "\n",
    "Después de iniciar nuestro clúster EMR, queremos poder enviarle trabajos. Aquí, adaptaremos el ejemplo de la canalización Spark MLlib que producimos en el Capítulo 3, De Modelo a Fábrica de Modelos, para analizar el conjunto de datos bancarios y enviar esto como un paso a nuestro clúster recién creado. Haremos esto como un script independiente de PySpark que actúa como un solo paso en nuestra aplicación, pero es fácil ampliar esto para crear aplicaciones mucho más complejas.\n",
    "\n",
    "Primero, tomaremos el código del Capítulo 3, De Modelo a Fábrica de Modelos, y realizaremos una buena refactorización basada en nuestras discusiones sobre buenas prácticas. Primero, podemos modularizar el código de manera más efectiva para que contenga una función que proporcione todos nuestros pasos de modelado (no todos los pasos se han reproducido aquí por brevedad). También hemos incluido un paso final que escribe los resultados del modelado en un archivo parquet:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "def model_bank_data(spark, input_path, output_path):\n",
    "    data = spark.read.format(\"csv\")\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(input_path)\n",
    "    data = data.withColumn('label', f.when((f.col(\"y\") == \"yes\"), 1).otherwise(0))\n",
    "    data.write.format('parquet')\\\n",
    "    .mode('overwrite')\\\n",
    "    .save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217ae71",
   "metadata": {},
   "source": [
    "Esta función model_bank_data:\n",
    "\n",
    "1. Lee un archivo CSV de datos bancarios.\n",
    "\n",
    "2. Crea una columna binaria label basada en si la respuesta y fue \"yes\".\n",
    "\n",
    "3. Guarda el resultado en formato Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b5461",
   "metadata": {},
   "source": [
    "Basándonos en esto, envolveremos todo el código principal de plantilla en una función principal que se pueda llamar en el punto de entrada del programa if __name__==\"__main__\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9761d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--input_path', ayuda='Ruta del bucket S3 para los datos de entrada. Se asume que es csv en este caso.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_path', ayuda='Ruta del bucket S3 para los datos de salida. Se asume que es parquet en este caso'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    # Create spark context\n",
    "    sc = SparkContext(\"local\", \"pipelines\")\n",
    "    # Get spark session\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('MLEIP Bank Data Classifier EMR Example')\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    model_bank_data(\n",
    "        spark,\n",
    "        input_path=args.input_path,#\"s3://mleip-emr-ml-simple/bank.csv\",\n",
    "        output_path=args.output_path#\"s3://mleip-emr-ml-simple/results.parquet\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba317b",
   "metadata": {},
   "source": [
    "Esta función main() está diseñada como punto de entrada de un script de PySpark, que probablemente se ejecuta en un entorno como Amazon EMR o localmente para procesar datos almacenados en Amazon S3.\n",
    "\n",
    "1. Parseo de argumentos desde la línea de comandos: \n",
    "\n",
    "    Usa el módulo argparse para definir dos argumentos que se deben pasar al ejecutar el script:\n",
    "\n",
    "        --input_path: ruta al archivo CSV (por ejemplo, en un bucket de S3).\n",
    "\n",
    "        --output_path: ruta donde se guardarán los datos procesados (como Parquet).\n",
    "\n",
    "    args.input_path y args.output_path almacenan los valores que se pasen al script desde la línea de comandos.\n",
    "\n",
    "2. Inicialización de Spark:\n",
    "\n",
    "Crea un contexto de Spark.\n",
    "\n",
    "- \"local\" indica que se ejecuta en modo local (no en un clúster).\n",
    "\n",
    "- \"pipelines\" es el nombre de la aplicación.\n",
    "\n",
    "- Crea una SparkSession, que es la entrada principal para usar la API de DataFrame en Spark.\n",
    "\n",
    "- La aplicación se llama \"MLEIP Bank Data Classifier EMR Example\".\n",
    "\n",
    "3. Llama a la función model_bank_data\n",
    "\n",
    "Usa los argumentos de entrada y salida proporcionados por el usuario para ejecutar la función model_bank_data, que:\n",
    "\n",
    "- Lee el CSV desde S3 (input_path).\n",
    "\n",
    "- Crea la columna label.\n",
    "\n",
    "- Escribe el resultado como Parquet en output_path.\n",
    "\n",
    "\n",
    "En definitiva, la función completa quedaria así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dddd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.ml.feature import StandardScaler, OneHotEncoder, StringIndexer, Imputer,VectorAssembler\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def model_bank_data(spark, input_path, output_path):\n",
    "    data = spark.read.format(\"csv\")\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(input_path)\n",
    "    data = data.withColumn('label', f.when((f.col(\"y\") == \"yes\"), 1).otherwise(0))\n",
    "    data.write.format('parquet')\\\n",
    "    .mode('overwrite')\\\n",
    "    .save(output_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--input_path', ayuda='Ruta del bucket S3 para los datos de entrada. Se asume que es csv en este caso.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_path', ayuda='Ruta del bucket S3 para los datos de salida. Se asume que es parquet en este caso'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    # Create spark context\n",
    "    sc = SparkContext(\"local\", \"pipelines\")\n",
    "    # Get spark session\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('MLEIP Bank Data Classifier EMR Example')\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    model_bank_data(\n",
    "        spark,\n",
    "        input_path=args.input_path,#\"s3://mleip-emr-ml-simple/bank.csv\",\n",
    "        output_path=args.output_path#\"s3://mleip-emr-ml-simple/results.parquet\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b82b4",
   "metadata": {},
   "source": [
    "Ahora, para enviar este script al clúster EMR que acabamos de crear, necesitamos averiguar el ID del clúster, que podemos obtener desde la interfaz de usuario de AWS o ejecutando el siguiente comando:\n",
    "\n",
    "`aws emr list-clusters --cluster-states WAITING`\n",
    "\n",
    "Luego, necesitamos enviar el script emr_sparkmllib.py a S3 para que el clúster pueda leerlo. Podemos crear un bucket de S3 llamado mleip-emr-ml-simple para almacenar este y otros artefactos utilizando ya sea la CLI o la consola de AWS (ver Capítulo 5, Patrones y Herramientas de Despliegue). Una vez copiado, estamos listos para los pasos finales.\n",
    "\n",
    "Ahora, debemos enviar el script usando el siguiente comando, reemplazando <CLUSTER_ID> con el ID del clúster que acabamos de crear. Después de unos minutos, el paso debería haberse completado y haber escrito los resultados en el archivo results.parquet en el mismo bucket de S3:\n",
    "\n",
    "aws emr add-steps \\\n",
    "--cluster-id <CLUSTER_ID> \\\n",
    "--steps Type=Spark, Name=\"Spark Application Step\",ActionOnFailure=CONTINUE,Args=\n",
    "[s3://mleip-emr-ml-simple/emr_sparkmllib.py,--input_path,s3://mleip-emr-ml-\n",
    "simple/bank.csv --output_path,s3://mleip-emr-ml-simple/results.parquet]\n",
    "\n",
    "¡Y eso es todo: así es como podemos empezar a desarrollar tuberías de ML con PySpark en la nube usando AWS EMR!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50556f",
   "metadata": {},
   "source": [
    "# Poner en marcha infraestructura sin servidor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a996dea",
   "metadata": {},
   "source": [
    "Siempre que hacemos cualquier tarea de aprendizaje automático o ingeniería de software, tenemos que ejecutar las tareas y cálculos requeridos en computadoras, a menudo con las redes, la seguridad y otros protocolos y software ya implementados, lo cual a menudo hemos referido previamente como nuestra infraestructura. Una gran parte de nuestra infraestructura son los servidores que usamos para ejecutar los cálculos reales. Esto podría parecer un poco extraño, así que comencemos hablando sobre la infraestructura sin servidor (¿cómo puede existir tal cosa?). Esta sección explicará este concepto y te mostrará cómo utilizarlo para escalar tus soluciones de aprendizaje automático.\n",
    "\n",
    "Serverless es un término un poco engañoso, ya que no significa que no haya servidores físicos ejecutando tus programas. Sin embargo, sí significa que los programas que estás ejecutando no deben considerarse como alojados de manera estática en una sola máquina, sino como instancias efímeras en otra capa encima del hardware subyacente.\n",
    "\n",
    "Los beneficios de las herramientas sin servidor para su solución de ML incluyen (pero no se limitan a) los siguientes:\n",
    "\n",
    "- `¡Sin servidores!` No subestimes el ahorro de tiempo y energía que puedes obtener al delegar la gestión de la infraestructura a tu proveedor de la nube.\n",
    "- `Simplified scaling`: Por lo general, es muy fácil definir el comportamiento de escalado de tus componentes serverless mediante el uso de un número máximo de instancias claramente definido, por ejemplo.\n",
    "- `Low barrier to entry`: Estos componentes suelen ser extremadamente fáciles de configurar y ejecutar, lo que permite a usted y a los miembros de su equipo centrarse en escribir código, lógica y modelos de alta calidad.\n",
    "- `Natural integration points`: Las herramientas sin servidor son a menudo agradables de usar para la transferencia entre otras herramientas y componentes. Su facilidad de configuración significa que puedes estar funcionando rápidamente con trabajos simples que transfieren datos o activan otros servicios en poco tiempo.\n",
    "- `Simplified serving`: Algunas herramientas serverless son excelentes para proporcionar una capa de servicio para tus modelos de ML. La escalabilidad y la baja barrera de entrada mencionadas anteriormente significan que puedes crear rápidamente un servicio muy escalable que brinde predicciones bajo solicitud o al ser activado por algún otro evento.\n",
    "\n",
    "Uno de los mejores y más utilizados ejemplos de funcionalidad sin servidor es AWS Lambda, que nos permite escribir programas en una variedad de lenguajes con una sencilla interfaz de navegador web o a través de nuestras herramientas de desarrollo habituales, y luego hacer que se ejecuten de manera completamente independiente de cualquier infraestructura que se haya configurado. Lambda es una solución increíble con una baja barrera de entrada para poner algún código en funcionamiento y escalarlo. Sin embargo, está muy orientado a la creación de APIs simples que pueden ser accedidas mediante una solicitud HTTP. Desplegar tu modelo de ML con Lambda es especialmente útil si apuntas a un sistema impulsado por eventos o solicitudes.\n",
    "\n",
    "Para ver esto en acción, construyamos un sistema básico que reciba datos de imagen entrantes mediante una solicitud HTTP con un cuerpo JSON y devuelva un mensaje similar que contenga la clasificación de los datos utilizando un modelo de scikit-learn preconstruido. Esta guía se basa en el ejemplo de AWS en https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/.\n",
    "\n",
    "Para esto, podemos ahorrar mucho tiempo aprovechando plantillas ya construidas y mantenidas como parte del marco de trabajo AWS Serverless Application Model (SAM) (https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/).\n",
    "\n",
    "\n",
    "Ahora, vamos a realizar los siguientes pasos para configurar una plantilla de despliegue de Lambda para alojar y servir un modelo de aprendizaje automático en una infraestructura sin servidor:\n",
    "\n",
    "Primero, debemos ejecutar el comando sam init y seleccionar la opción de Plantillas de Inicio Rápido de AWS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2aabb",
   "metadata": {},
   "source": [
    "# Containerizing at scale with Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81fc7de",
   "metadata": {},
   "source": [
    "Ya hemos cubierto cómo usar contenedores para construir y desplegar nuestras soluciones de ML. El siguiente paso es entender cómo orquestar y gestionar varios contenedores para desplegar y ejecutar aplicaciones a gran escala. Aquí es donde entra la herramienta de código abierto K8s.\n",
    "\n",
    "K8s es una herramienta extremadamente poderosa que ofrece una variedad de funcionalidades diferentes que nos ayudan a crear y gestionar aplicaciones en contenedores muy escalables, incluyendo (pero no limitado a) lo siguiente:\n",
    "\n",
    "- `Load Balancing`: K8s gestionará el enrutamiento del tráfico entrante hacia tus contenedores para que la carga se distribuya de manera equitativa.\n",
    "- `Horizontal Scaling`:K8s proporciona interfaces simples para que puedas controlar la cantidad de instancias de contenedores que tienes en cualquier momento, lo que te permite escalar masivamente si es necesario.\n",
    "- `Self Healing`: Existe una gestión integrada para reemplazar o reprogramar los componentes que no superan las verificaciones de estado.\n",
    "- `Automated Rollbacks`: K8s almacena el historial de tu sistema para que puedas revertir a una versión anterior que funcione si algo sale mal.\n",
    "\n",
    "Todas estas características ayudan a garantizar que sus soluciones desplegadas sean robustas y capaces de funcionar según lo requerido en todas las circunstancias. K8s está diseñado para asegurar que las características anteriores estén integradas desde el inicio mediante el uso de una arquitectura de microservicios, con un plano de control que interactúa con nodos (servidores), cada uno de los cuales aloja pods (uno o más contenedores) que ejecutan los componentes de su aplicación.\n",
    "\n",
    "Lo más importante que K8s te ofrece es la capacidad de escalar tu aplicación en función de la carga mediante la creación de réplicas de la solución base. Esto es extremadamente útil si estás construyendo servicios con endpoints de API que podrían enfrentar aumentos en la demanda en diferentes momentos. Para aprender sobre algunas de las formas en que puedes hacer esto, consulta https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment:\n",
    "\n",
    "Kubeflow se presenta a sí mismo como el conjunto de herramientas de ML para K8s (https://www.kubeflow.org/), por lo que, como ingenieros de ML, tiene sentido que estemos al tanto de esta solución en rápido desarrollo. Esta es una herramienta muy emocionante y un área de desarrollo activa. El concepto de escalado horizontal para K8s generalmente todavía se aplica aquí, pero Kubeflow proporciona algunas herramientas estandarizadas para convertir los pipelines que construyes en recursos estándar de K8s, los cuales luego pueden gestionarse y asignarse recursos de las maneras descritas anteriormente. Esto puede ayudar a reducir el código repetitivo y nos permite a nosotros, como ingenieros de ML, centrarnos en construir nuestra lógica de modelado en lugar de configurar la infraestructura, aunque K8s ya sea una buena abstracción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f3ccb",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618b120",
   "metadata": {},
   "source": [
    "En este capítulo, analizamos cómo tomar las soluciones de ML que hemos estado construyendo en los capítulos anteriores y consideramos cómo escalarlas a volúmenes de datos más grandes o a un mayor número de solicitudes de predicciones. Para esto, nos centramos principalmente en Apache Spark, ya que es el motor de propósito general más popular para la computación distribuida. Durante nuestra discusión sobre Apache Spark, revisitamos algunos patrones de codificación y sintaxis que habíamos usado previamente en este libro. Al hacerlo, desarrollamos una comprensión más profunda de cómo y por qué realizar ciertas acciones al desarrollar en PySpark. Discutimos el concepto de UDFs en detalle y cómo estos pueden usarse para crear flujos de trabajo de ML altamente escalables.\n",
    "\n",
    "Después de esto, exploramos cómo trabajar con Spark en la nube, específicamente a través del servicio Elastic MapReduce (EMR) proporcionado por AWS. Luego, analizamos algunas de las otras formas en que podemos escalar nuestras soluciones; es decir, mediante arquitecturas sin servidor y escalado horizontal con contenedores. En el primer caso, recorrimos cómo construir un servicio para ofrecer un modelo de ML utilizando AWS Lambda. Esto utilizó plantillas estándar proporcionadas por el marco de Gestión de Aplicaciones Serverless de AWS. Finalmente, proporcionamos una visión general de cómo usar K8s y Kubeflow para escalar horizontalmente tuberías de ML, así como algunos de los otros beneficios de usar estas herramientas.\n",
    "\n",
    "En el próximo capítulo, reuniremos muchos aspectos de nuestro nuevo conocimiento en ingeniería de ML para construir un microservicio de pronóstico, basándonos en el ejemplo de modelo básico mostrado en el Capítulo 1, Introducción a la Ingeniería de ML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
