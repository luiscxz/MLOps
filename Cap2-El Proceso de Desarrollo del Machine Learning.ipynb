{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33dc0f1a",
   "metadata": {},
   "source": [
    "**Tabla de contenido**\n",
    "\n",
    "- [Requrimientos](#Requerimientos)\n",
    "- [Configurando nuestras herramientas](#Configurando-nuestras-herramientas)\n",
    "- [Configurando una cuenta de AWS](#Configurando-una-cuenta-de-AWS)\n",
    "- [Concepto a solución en cuatro pasos](#Concepto-a-solucion-en-cuatro-pasos)\n",
    "    - [Descubrir](#Descubrir)\n",
    "    - [Usando historias de usuario](#Usando-historias-de-usuario)\n",
    "    - [El juego](#El-juego)\n",
    "    - [Desarrollar](#Desarrollar)\n",
    "        - [Seleccionando una metodología de desarrollo de software](#Seleccionando-una-metodologia-de-desarrollo-de-software)\n",
    "        - [Gestión de paquetes (conda y pip)](#Gestion-de-paquetes-conda-y-pip)\n",
    "        - [Control de versiones de código](#Control-de-versiones-de-codigo)\n",
    "        - [Gitflow Workflow](#Gitflow-Workflow)\n",
    "        - [Control de versiones del modelo](#control-de-versiones-del-modelo)\n",
    "    - [Desplegar](#Desplegar)\n",
    "        - [Conocer sus opciones de implementación](#Conocer-sus-opciones-de-implementacion)\n",
    "        - [DevOps y MLOps](#DevOps-y-MLOps)\n",
    "- [Resumen](#Resumen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98b0fb",
   "metadata": {},
   "source": [
    "En este capítulo, definiremos cómo se puede dividir el trabajo para cualquier proyecto de ingeniería de software de Aprendizaje Automático (ML) exitoso. Básicamente, responderemos a la pregunta de cómo se organiza realmente la realización de un proyecto ML exitoso. No solo discutiremos el proceso y el flujo de trabajo, sino que también estableceremos las herramientas que necesitarás para cada etapa del proceso y destacaremos algunas prácticas recomendadas importantes con ejemplos de código ML reales.\n",
    "\n",
    "Específicamente, este capítulo cubrirá el concepto de un flujo de trabajo de descubrir, jugar, desarrollar y desplegar para tus proyectos de ML, las herramientas de desarrollo apropiadas y su configuración e integración para un proyecto exitoso. También cubriremos estrategias de control de versiones y su implementación básica, configurando la Integración Continua/Despliegue Continuo (CI/CD) para tu proyecto de ML. Además, presentaremos algunos entornos de ejecución potenciales. Al final de este capítulo, estarás preparado para el éxito en tu proyecto de ingeniería ML en Python. Esta es la base sobre la cual construiremos todo en los capítulos subsecuentes.\n",
    "\n",
    "Como de costumbre, concluiremos el capítulo resumiendo los puntos principales y destacando lo que esto significa mientras trabajamos a través del resto del libro.\n",
    "\n",
    "Finalmente, también es importante señalar que aunque enmarcaremos la discusión aquí en términos de desafíos de ML, la mayor parte de lo que aprenderás en este capítulo también se puede aplicar a otros proyectos de ingeniería de software en Python. Mi esperanza es que la inversión en desarrollar estos conceptos fundamentales en detalle sea algo que puedas aprovechar una y otra vez en todo tu trabajo.\n",
    "\n",
    "Exploraremos todo esto en las secciones siguientes:\n",
    "\n",
    "- Configurando nuestras herramientas\n",
    "- Concepto a solución en cuatro pasos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab9a0a1",
   "metadata": {},
   "source": [
    "# Requerimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb84f70",
   "metadata": {},
   "source": [
    "- Anaconda\n",
    "- PyCharm Community Edition\n",
    "- Git\n",
    "- Jupyter NotebookPySpark\n",
    "- fbprophet\n",
    "- scikit-learn\n",
    "- MLflow\n",
    "- pytest\n",
    "- Flake8\n",
    "\n",
    "También necesitarás lo siguiente:\n",
    "\n",
    "- Una cuenta de Atlassian Jira. Hablaremos más de esto más adelante en el capítulo, pero puedes registrarte para obtener una de forma gratuita en https://www.atlassian.com/software/jira/free.\n",
    "- Una cuenta de AWS. Esto también se cubrirá en el capítulo, pero puedes registrarte para una cuenta en https://aws.amazon.com/. Necesitarás agregar detalles de pago para registrarte en AWS, pero todo lo que haremos en este libro solo requerirá soluciones del nivel gratuito.\n",
    "\n",
    "Los pasos técnicos en este capítulo se ejecutaron todos en una máquina Linux que ejecuta Ubuntu 24.04.3 LTS con un perfil de usuario que tenía derechos de administrador. Si está ejecutando los pasos en un sistema diferente, es posible que deba consultar la documentación de esa herramienta específica si los pasos no funcionan como se esperaba. Incluso si ese es el caso, la mayoría de los pasos serán casi exactamente los mismos o muy similares para la mayoría de los sistemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0113d",
   "metadata": {},
   "source": [
    "# Configurando nuestras herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f8ce1",
   "metadata": {},
   "source": [
    "Para prepararnos para el trabajo en el resto de este capítulo, y de hecho en el resto del libro, será útil establecer algunas herramientas. A un alto nivel, necesitamos lo siguiente:\n",
    "- Un lugar para codificar\n",
    "- Algo para rastrear nuestros cambios de código\n",
    "- Algo para ayudar a gestionar nuestras tareas\n",
    "- Un lugar para aprovisionar infraestructura y desplegar nuestra solución\n",
    "\n",
    "Veamos cómo abordar cada uno de ellos por turno:\n",
    "\n",
    "- `Un lugar para codificar`: Primero, aunque el arma elegida para codificar por los científicos de datos es, por supuesto, Jupyter Notebook (existen otras soluciones disponibles), una vez que comiences a hacer la transición hacia la ingeniería de ML, será importante tener a mano un Entorno de Desarrollo Integrado (IDE). Un IDE es básicamente una aplicación que viene con una serie de herramientas y capacidades integradas para ayudarte a desarrollar el mejor software que puedas. `PyCharm` es un excelente ejemplo para los desarrolladores de Python y viene con una amplia variedad de complementos, extensiones e integraciones útiles para el ingeniero de aprendizaje automático.\n",
    "\n",
    "- `Algo para rastrear los cambios de código`: Lo siguiente en la lista es un sistema de control de versiones de código. En este libro, utilizaremos GitHub, pero hay una variedad de soluciones, todas disponibles gratuitamente, que se basan en la misma tecnología de código abierto Git subyacente. Las secciones posteriores discutirán cómo usar estas como parte de tu flujo de trabajo de desarrollo, pero primero, si no tienes un sistema de control de versiones configurado, puedes navegar a github.com y crear una cuenta gratuita. El archivo .gitignore le indica a su distribución de Git que ignore ciertos tipos de archivos que en general no son importantes para el control de versiones. Depende de usted decidir si quiere que el repositorio sea público o privado y qué licencia desea usar. El repositorio de este libro utiliza la licencia MIT.\n",
    "\n",
    "Una vez que hayas configurado tu IDE y sistema de control de versiones, necesitas hacer que se comuniquen entre sí utilizando los complementos de Git proporcionados con PyCharm. Esto es tan simple como navegar a VCS | Habilitar Integración de Control de Versiones y seleccionar Git.\n",
    "\n",
    "- `Algo para ayudar a gestionar nuestras tareas`: Ahora estás listo para escribir en Python y rastrear los cambios de tu código, pero ¿estás listo para gestionar o participar en un proyecto complejo con otros miembros del equipo? Para esto, a menudo es útil tener una solución donde puedas rastrear tareas, problemas, errores, historias de usuario y otra documentación y elementos de trabajo. También ayuda si esto cuenta con buenos puntos de integración con las otras herramientas que utilizarás. En este libro, usaremos Jira como un ejemplo de esto. Si navegas a https://www.atlassian.com/software/jira, puedes crear una cuenta gratuita en la nube de Jira y luego seguir el tutorial interactivo dentro de la solución para configurar tu primer tablero y crear algunas tareas.\n",
    "\n",
    "- `Un lugar para aprovisionar infraestructura y desplegar nuestra solución`: Todo lo que acabas de instalar y configurar son herramientas que realmente ayudarán a llevar tu flujo de trabajo y prácticas de desarrollo de software al siguiente nivel. La última pieza del rompecabezas es contar con las herramientas, tecnologías e infraestructura disponibles para desplegar la solución final. La gestión de la infraestructura informática para aplicaciones fue (y a menudo todavía lo es) la provisión de equipos de infraestructura dedicados, pero con la llegada de las nubes públicas, ha habido una verdadera democratización de esta capacidad para las personas que trabajan en el espectro de roles de software. En particular, la ingeniería moderna de ML depende mucho de la implementación exitosa de tecnologías en la nube, generalmente a través de los principales proveedores de nubes públicas como Amazon Web Services (AWS), Microsoft Azure o Google Cloud Platform (GCP). Este libro utilizará herramientas encontradas en el ecosistema de AWS, pero todas las herramientas y técnicas que encontrarás aquí tienen equivalentes en otras nubes.\n",
    "\n",
    "El lado negativo de la democratización de las capacidades que trae la nube es que los equipos que poseen el despliegue de sus soluciones tienen que adquirir nuevas habilidades y entendimiento. Soy un firme creyente en el principio de que si lo construyes, lo posees y lo ejecutas tanto como sea posible, pero esto significa que, como ingeniero de ML, tendrás que sentirte cómodo con una serie de herramientas y principios potencialmente nuevos, así como ser responsable del rendimiento de tu solución desplegada. Con gran poder viene una gran responsabilidad y todo eso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c0c323",
   "metadata": {},
   "source": [
    "# Configurando una cuenta de AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126e435",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, no tienes que usar AWS, pero eso es lo que vamos a utilizar a lo largo de este libro. Una vez que esté configurado aquí, puedes usarlo para todo lo que haremos:\n",
    "\n",
    "- Para configurar una cuenta de AWS, navega a aws.amazon.com y selecciona Crear Cuenta. Tendrás que agregar algunos detalles de pago, pero todo lo que mencionamos en este libro se puede explorar a través del nivel gratuito de AWS, donde no incurrirás en costos por debajo de un umbral de consumo establecido.\n",
    "- Una vez que hayas creado tu cuenta, puedes navegar a la Consola de Administración de AWS, donde puedes ver todos los servicios que están disponibles para ti.\n",
    "- Finalmente, no habría ingeniería de ML sin modelos de ML. Así que, la última pieza de software que debes instalar es una que te ayude a rastrear y servir tus modelos de manera consistente. Para esto, utilizaremos MLflow, una plataforma de código abierto de Databricks y bajo la supervisión de la Fundación Linux. Configurar MLflow localmente es muy simple; todo lo que tienes que hacer es encontrar tu terminal UNIX normal y ejecutar el siguiente código: `pip install mlflow`\n",
    "\n",
    "Más adelante en este capítulo, echaremos un vistazo a la diferencia entre pip y conda, pero el punto clave aquí es que instalar MLflow es realmente simple. Discutiremos la instalación de MLflow en la nube más adelante en el libro, pero para la mayoría de los ejemplos en el libro, puedes trabajar localmente en la primera instancia.\n",
    "\n",
    "Con nuestra cuenta de AWS lista para usar, examinemos los cuatro pasos que cubren todo el proceso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8de90f",
   "metadata": {},
   "source": [
    "# Concepto a solucion en cuatro pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02493549",
   "metadata": {},
   "source": [
    "Todos los proyectos de ML son únicos de alguna manera: la organización, los datos, las personas y las herramientas y técnicas empleadas nunca serán exactamente las mismas para dos proyectos. Esto es bueno, ya que significa progreso así como la variedad natural que hace de este un espacio tan divertido para trabajar.\n",
    "\n",
    "Dicho esto, no importa los detalles, en términos generales, todos los proyectos de ML exitosos tienen muchas cosas en común. Requieren la traducción de un problema de negocio en un problema técnico, mucha investigación y comprensión, pruebas de concepto, análisis, iteraciones, consolidación del trabajo, construcción del producto final y despliegue en un entorno apropiado. ¡Ese es el ingeniero de ML en pocas palabras!\n",
    "\n",
    "| Etapa    | Resultados                                                                 |\n",
    "|----------|-----------------------------------------------------------------------------|\n",
    "| Descubrir | - Claridad sobre la pregunta de negocio  <br> - Argumentos claros para ML sobre otro enfoque  <br> - Definición de los KPIs y métricas que se quieren optimizar  <br> - Un esquema de la ruta hacia el valor |\n",
    "| Explorar  | - Entendimiento detallado de los datos  <br> - Prueba de concepto funcional  <br> - Acuerdo sobre el modelo/algoritmo/lógica que resolverá el problema  <br> - Evidencia de que la solución es viable dentro de escenarios de recursos realistas  <br> - Evidencia de que se puede lograr un buen ROI |\n",
    "| Desarrollar | - Una solución funcional que pueda ser alojada en infraestructura apropiada y disponible  <br> - Resultados de pruebas exhaustivas y métricas de desempeño (para algoritmos y software)  <br> - Estrategia acordada de reentrenamiento y despliegue del modelo  <br> - Pruebas unitarias, de integración y de regresión  <br> - Empaquetado de la solución y canalizaciones |\n",
    "| Desplegar | - Un proceso de despliegue funcional y probado  <br> - Infraestructura aprovisionada con seguridad y características de rendimiento adecuadas  <br> - Procesos de reentrenamiento y gestión del modelo  <br> - ¡Una solución de extremo a extremo funcionando! |\n",
    "\n",
    "`NOTA IMPORTANTE` Puede que pienses que un ingeniero de ML solo necesita considerar las dos últimas etapas, desarrollar y desplegar, y que las etapas anteriores son responsabilidad del científico de datos o incluso de un analista de negocios. De hecho, nos centraremos principalmente en estas etapas a lo largo de este libro y esta división del trabajo puede funcionar muy bien. Sin embargo, es crucialmente importante que si vas a construir una solución de ML entiendas todos los motivos y pasos de desarrollo que han precedido – no construirías un nuevo tipo de cohete sin entender primero a dónde quieres ir, ¿verdad?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05313bf",
   "metadata": {},
   "source": [
    "## Descubrir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84cdd3",
   "metadata": {},
   "source": [
    "Antes de comenzar a trabajar para construir cualquier solución, es vitalmente importante que entiendas el problema que estás tratando de resolver. Esta actividad a menudo se denomina descubrimiento en el análisis de negocios y es crucial si tu proyecto de ML va a tener éxito. Las cosas clave que debes hacer durante la fase de descubrimiento son las siguientes:\n",
    "\n",
    "- ¡Habla con el cliente! Y luego háblales de nuevo: Debes entender en detalle los requisitos del usuario final si vas a diseñar y construir el sistema correcto.\n",
    "- Documenta todo: Serás evaluado sobre cuán bien cumples con los requisitos, así que asegúrate de que todos los puntos clave de tu discusión estén documentados y firmados por miembros de tu equipo y el cliente o su representante apropiado.\n",
    "\n",
    "- Define las métricas que importan: Al principio de un proyecto, es muy fácil dejarse llevar y sentir que se puede resolver cualquier problema con la increíble nueva herramienta que vas a construir. Lucha contra esta tendencia con la mayor intensidad posible, ya que puede causar grandes dolores de cabeza más adelante. En su lugar, orienta tus conversaciones hacia la definición de una sola métrica o un número muy pequeño de métricas que definan cómo se verá el éxito.\n",
    "\n",
    "- ¡Empieza a averiguar dónde viven los datos!: Si puedes comenzar a determinar qué tipo de sistemas tendrás que acceder para obtener los datos que necesitas, esto te ahorrará tiempo más adelante y puede ayudarte a encontrar cualquier problema importante antes de que descarrile tu proyecto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03f5ec",
   "metadata": {},
   "source": [
    "## Usando historias de usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb81a3",
   "metadata": {},
   "source": [
    "Una vez que hayas hablado con el cliente (unas cuantas veces), puedes comenzar a definir algunas historias de usuario. Las historias de usuario son expresiones concisas y formateadas de manera consistente de lo que el usuario o cliente desea ver y los criterios de aceptación para esa característica o unidad de trabajo. Por ejemplo, podemos querer definir una historia de usuario basada en el ejemplo del viaje en taxi del Capítulo 1, Introducción a la Ingeniería de ML: Como usuario de nuestro servicio web interno, quiero ver viajes en taxi anómalos y poder investigarlos más a fondo.\n",
    "\n",
    "¡Comencemos! \n",
    "1. Para agregar esto en Jira, selecciona el botón Crear. \n",
    "2. A continuación, selecciona Historia. \n",
    "3. Luego, completa los detalles que consideres apropiados. \n",
    "\n",
    "¡Ahora has agregado una historia de usuario a tu herramienta de gestión de trabajo! Esto te permite hacer cosas como crear nuevas tareas y vinculadas a esta historia de usuario o actualizar su estado a medida que avanza tu proyecto.\n",
    "\n",
    "Las fuentes de datos que utilizas son especialmente cruciales para entender. Como sabes, basura entrante, basura saliente, o incluso peor, sin datos, ¡no hay avance! Las preguntas particulares que debes responder sobre los datos se centran principalmente en el acceso, la tecnología, la calidad y la relevancia.\n",
    "\n",
    "Para el acceso y la tecnología, estás tratando de anticipar cuánto trabajo tendrán que hacer los ingenieros de datos para iniciar su pipeline de trabajo y cuánto esto retrasará el resto del proyecto. Por lo tanto, es crucial que hagas esto bien.\n",
    "\n",
    "Un buen ejemplo sería si descubres rápidamente que la mayor parte de los datos que necesitarás se encuentran en un sistema financiero interno legado sin APIs modernas reales y sin un mecanismo de solicitud de acceso para miembros no del equipo de finanzas. Si su backend principal está en las instalaciones y necesitas migrar datos financieros bloqueados a la nube, pero esto pone nerviosa a tu empresa, entonces sabes que tienes mucho trabajo por hacer antes de escribir una línea de código. Si los datos ya están en tu lago de datos empresarial al que tu equipo ya tiene acceso, entonces estás en una mejor posición. Cualquier desafío es superable si la propuesta de valor es lo suficientemente fuerte, pero descubrir todo esto temprano te ahorrará tiempo, energía y dinero más adelante.\n",
    "\n",
    "La relevancia es un poco más difícil de determinar antes de comenzar, pero puedes empezar a tener una idea. Por ejemplo, si deseas realizar la previsión de inventario que discutimos en el Capítulo 1, Introducción a la Ingeniería de ML, ¿necesitas obtener información de cuentas de clientes? Si quieres crear el clasificador de clientes premium o no premium como objetivos de marketing, también mencionado en el Capítulo 1, Introducción a la Ingeniería de ML, ¿necesitas tener datos sobre feeds de redes sociales? La pregunta sobre qué es relevante a menudo será menos clara que estas, pero una cosa importante a recordar es que siempre puedes volver a ello si realmente te perdiste algo importante. Estás tratando de capturar las decisiones de diseño más importantes desde el principio, por lo que el sentido común y la participación de muchos interesados y expertos en la materia serán de gran ayuda.\n",
    "\n",
    "La calidad de los datos es algo que puedes intentar anticipar un poco antes de avanzar en tu proyecto, haciendo algunas preguntas a los usuarios actuales o consumidores de los datos o a aquellos involucrados en sus procesos de entrada. Sin embargo, para obtener una comprensión más cuantitativa, a menudo solo necesitarás que tus científicos de datos trabajen con los datos de manera práctica.\n",
    "\n",
    "En la siguiente sección, veremos cómo desarrollamos soluciones de aprendizaje automático de prueba de concepto en la fase más intensiva en investigación, el juego.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980de5e",
   "metadata": {},
   "source": [
    "## El juego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc010e2",
   "metadata": {},
   "source": [
    "En la etapa de desarrollo del proyecto, su objetivo es determinar si resolver la tarea, incluso a nivel de prueba de concepto, es factible. Para hacer esto, podría emplear las técnicas habituales de análisis de datos exploratorio y modelado explicativo que mencionamos en el capítulo anterior antes de pasar a crear un modelo de aprendizaje automático que haga lo que necesita.\n",
    "\n",
    "En esta parte del proceso, no te preocupas demasiado por los detalles de implementación, sino por explorar los reinos de las posibilidades y obtener una comprensión profunda de los datos y del problema que va más allá del trabajo de descubrimiento inicial. Dado que el objetivo aquí no es crear un código listo para producción o construir herramientas reutilizables, no debes preocuparte por si el código que estás escribiendo es de la más alta calidad o utiliza patrones sofisticados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b36616",
   "metadata": {},
   "source": [
    "## Desarrollar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db021d",
   "metadata": {},
   "source": [
    "Como hemos mencionado algunas veces ya, uno de los objetivos de este libro es hacerte reflexionar sobre el hecho de que estás construyendo productos de software que simplemente tienen ML en ellos. Esto significa una curva de aprendizaje empinada para algunos de nosotros que venimos de antecedentes más matemáticos y algorítmicos. Esto puede parecer intimidante, pero no desesperes. La buena noticia es que podemos reutilizar muchas de las mejores prácticas y técnicas perfeccionadas a través de la comunidad de ingeniería de software a lo largo de varias décadas. No hay nada nuevo bajo el sol.\n",
    "\n",
    "Esta sección explora varias de esas metodologías, procesos y consideraciones que se pueden emplear en la fase de desarrollo de nuestros proyectos de ingeniería de aprendizaje automático.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033af260",
   "metadata": {},
   "source": [
    "### Seleccionando una metodologia de desarrollo de software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab15b2",
   "metadata": {},
   "source": [
    "Una de las primeras cosas que podríamos y deberíamos replicar sin vergüenza como ingenieros de ML son las metodologías de desarrollo de software que se utilizan en proyectos en todo el mundo. Una categoría de estas, a menudo llamada Cascada, abarca flujos de trabajo de proyectos que encajan bastante naturalmente con la idea de construir algo complejo (piensa en un edificio o un automóvil). En las metodologías en cascada (Waterfall), hay fases de trabajo distintas y secuenciales, cada una con un conjunto claro de resultados que se necesitan antes de pasar a la siguiente fase. Por ejemplo, un proyecto típico en cascada puede tener fases que abarcan, en términos generales, la recopilación de requisitos, análisis, diseño, desarrollo, pruebas y despliegue (¿te suena familiar?). Lo clave es que en un proyecto con un enfoque en cascada, cuando estás en la fase de recopilación de requisitos, solo deberías estar trabajando en la recopilación de requisitos, cuando estés en la fase de pruebas, solo deberías estar trabajando en pruebas, y así sucesivamente. Discutiremos los pros y los contras de esto para ML en los próximos párrafos, después de introducir otro conjunto de metodologías.\n",
    "\n",
    "El otro conjunto de metodologías, denominado Ágil, inició su vida tras la introducción del Manifiesto Ágil en 2001 (https://agilemanifesto.org/). En el corazón del desarrollo Ágil están las ideas de flexibilidad, iteración y actualizaciones incrementales, fallar rápido y adaptarse a los requisitos cambiantes. Si vienes de un fondo de investigación o científico, este concepto de flexibilidad y adaptabilidad basado en resultados y nuevos hallazgos puede sonar familiar.\n",
    "\n",
    "Lo que puede no ser tan familiar para usted si tiene este tipo de formación científica o académica es que aún puede abrazar estos conceptos dentro de un marco relativamente estricto que se centra en los resultados de entrega. Las metodologías de desarrollo ágil de software se centran en encontrar el equilibrio entre la experimentación y la entrega. Esto a menudo se logra introduciendo los conceptos de ceremonias (como Scrums y Retrospectivas de Sprint) y roles (como Scrum Master y Product Owner).\n",
    "\n",
    "Además de esto, dentro del desarrollo ágil, hay dos variantes que son extremadamente populares: Scrum y Kanban. Los proyectos de Scrum se centran en unidades cortas de trabajo llamadas Sprints, donde la idea es tomar adiciones al producto desde la ideación hasta el despliegue en ese corto período de tiempo. En Kanban, la idea principal es lograr un flujo constante de tareas desde un backlog organizado hasta el trabajo en progreso y el trabajo completado.\n",
    "\n",
    "Todas estas metodologías (y muchas más además) tienen sus méritos y sus desventajas. No tienes que estar casado con ninguna de ellas; puedes cambiar y alternar entre ellas. Por ejemplo, en un proyecto de ML, puede tener sentido realizar algunas actividades posteriores al despliegue que se centren en mantener un servicio ya existente (a veces denominado actividad de negocio como de costumbre) como mejoras adicionales en el modelo u optimizaciones de software en un marco Kanban. Puede tener sentido realizar la entrega principal de tu cuerpo de trabajo en Sprints con resultados muy claros. Pero puedes cambiar y experimentar para ver qué se adapta mejor a tus casos de uso, tu equipo y tu organización.\n",
    "\n",
    "¿Pero qué hace que aplicar este tipo de flujos de trabajo a los proyectos de ML sea diferente? ¿En qué debemos pensar en este mundo de ML que no pensábamos antes? Bueno, algunos de los puntos clave son los siguientes:\n",
    "\n",
    "- No sabes lo que no sabes: No puedes saber si podrás resolver el problema hasta que hayas visto los datos. La ingeniería de software tradicional no depende tan críticamente de los datos que fluirán a través del sistema como lo hace la ingeniería de ML. Podemos saber cómo resolver un problema en principio, pero si los datos apropiados no existen en cantidad suficiente o son de mala calidad, entonces no podemos resolver el problema en la práctica.\n",
    "\n",
    "- Tu sistema está vivo: Si construyes un sitio web clásico, con su base de datos backend, un frontend brillante, un balanceo de carga asombroso y otras características, entonces, de manera realista, si el recurso está disponible, puede funcionar indefinidamente. Nada fundamental cambia sobre el sitio web y sigue funcionando con el tiempo. Los clics aún se traducen en acciones y la navegación por las páginas sigue ocurriendo de la misma manera. Ahora considera poner contenido publicitario generado por ML en función de perfiles típicos de usuarios. ¿Cuál es un perfil de usuario típico y cambia eso con el tiempo? Con más tráfico y más usuarios, ¿se convierten en la nueva normalidad comportamientos que nunca vimos antes? Tu sistema está aprendiendo todo el tiempo y eso conlleva los problemas de deriva del modelo y cambio de distribución, así como escenarios de actualización y reversión más complejos.\n",
    "\n",
    "\n",
    "Dada estas cuestiones, en la próxima sección, intentaremos entender qué metodologías de desarrollo pueden ayudarnos al construir nuestras soluciones de aprendizaje automático. En la tabla, podemos ver algunas ventajas y desventajas de cada una de estas metodologías ágiles para diferentes etapas y tipos de proyectos de ingeniería de aprendizaje automático:\n",
    "| Metodología | Pros | Contras |\n",
    "|-------------|------|---------|\n",
    "| Agile       | Se espera flexibilidad. <br> Ciclos más rápidos de desarrollo a despliegue. | Si no se gestiona bien, puede haber fácilmente desviación del alcance. <br> Kanban o Sprints pueden no funcionar bien para algunos proyectos. |\n",
    "| Waterfall     | Camino más claro hacia el despliegue. <br> Etapas y responsabilidades de tareas más definidas. | Falta de flexibilidad. <br> Mayores cargas administrativas. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75357ebb",
   "metadata": {},
   "source": [
    "### Gestion de paquetes conda y pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e042ae",
   "metadata": {},
   "source": [
    "Si te dijera que escribas un programa que hiciera cualquier cosa en ciencia de datos o aprendizaje automático sin usar bibliotecas o paquetes y solo con Python puro, probablemente encontrarías esto bastante difícil de lograr en un tiempo razonable y, ¡increíblemente aburrido! Esto es algo bueno. Una de las características realmente poderosas del desarrollo de software en Python es que puedes aprovechar un ecosistema extenso de herramientas y capacidades de manera relativamente fácil. El lado opuesto de esto es que sería muy fácil que la gestión de las dependencias de tu código se convirtiera en una tarea muy complicada y difícil de replicar. Aquí es donde entran los administradores de paquetes y entornos como pip y conda.\n",
    "\n",
    "pip es el gestor de paquetes estándar en Python y el recomendado para su uso por la Autoridad de Paquetes de Python. Recupera e instala paquetes de Python desde PyPI, el Índice de Paquetes de Python. pip es muy fácil de usar y a menudo es la forma sugerida de instalar paquetes en tutoriales y libros.\n",
    "\n",
    "conda es el gestor de paquetes y entornos que viene con las distribuciones de Python Anaconda y Miniconda. Una de las principales fortalezas de conda es que, aunque proviene del ecosistema de Python, y tiene excelentes capacidades en ese ámbito, en realidad es un gestor de paquetes más general. Así, si tu proyecto requiere dependencias fuera de Python (las bibliotecas numpy y scipy son buenos ejemplos), aunque pip puede instalar estas, no puede rastrear todas las dependencias que no son de Python, ni gestionar sus versiones. Con conda, esto se resuelve.\n",
    "\n",
    "También puedes usar pip dentro de entornos conda, así que puedes intentar obtener lo mejor de ambos mundos o usar lo que necesites para tu proyecto. Dicho esto, sugiero encarecidamente apegarse a instalar paquetes con el comando conda tanto como sea posible, debido al punto sobre las dependencias no-Python, y también por consistencia. A lo largo de este libro, utilizaremos conda tanto para gestionar nuestros entornos de Python como para la gestión e instalación de paquetes (ya has visto esto en secciones anteriores, por ejemplo, cuando instalamos MLflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834f5b4",
   "metadata": {},
   "source": [
    "### Control de versiones de codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031254c",
   "metadata": {},
   "source": [
    "Si vas a escribir código para sistemas reales, casi con seguridad lo harás como parte de un equipo. También te será más fácil si puedes tener un registro de auditoría limpio de los cambios, ediciones y actualizaciones para que puedas ver cómo se ha desarrollado la solución. Finalmente, querrás separar de manera clara y segura las versiones estables de la solución que estás construyendo y que pueden ser implementadas, frente a las versiones de desarrollo más transitorias. Afortunadamente, todo esto está a cargo de los sistemas de control de versiones de código fuente, el más popular de los cuales es Git.\n",
    "\n",
    "La presencia de una estrategia para utilizar sistemas de control de versiones puede ser a menudo un factor clave que diferencia los aspectos de ciencia de datos y ingeniería de ML de un proyecto. A veces puede ser excesivo definir una estrategia estricta de Git para etapas exploratorias y de modelado básico (descubrir y jugar), pero si quieres diseñar algo para su implementación (y estás leyendo este libro, así que es probable que estés pensando en esto), entonces es fundamentalmente importante.\n",
    "\n",
    "Genial, pero ¿qué queremos decir con una estrategia de Git? Bueno, imaginemos que intentamos desarrollar nuestra solución sin una dirección compartida sobre cómo organizar la versionado y el código. El ingeniero de ML quiere comenzar a construir parte del código de ciencia de datos en un pipeline de Spark MLlib (más sobre esto más adelante), así que crea una rama main llamada pipeline1spark:\n",
    "\n",
    "- git checkout -b pipeline1spark\n",
    "\n",
    "Luego comienza a trabajar en la rama y escribe un buen código en un nuevo archivo llamado pipeline.py\n",
    "\n",
    "Genial, ha hecho un excelente progreso al traducir parte del código de sklearn a Spark, lo cual se consideró más apropiado para el caso de uso. Luego sigue trabajando en esta rama porque tiene todas sus adiciones, y piensa que es mejor hacer todo en un solo lugar. Cuando quiere enviar la rama al repositorio remoto, ejecuta los siguientes comandos:\n",
    "\n",
    "- git push origin pipelinespark\n",
    "\n",
    "El ingeniero de ML B llega y quiere usar el código del ingeniero de ML A y construir algunos pasos adicionales alrededor de ello. Sabe que ella tiene una rama con este trabajo, así que sabe lo suficiente de Git para crear otra rama con su código en ella, a la que llama pipeline:\n",
    "\n",
    "- git pull origin pipelinspark\n",
    "- git checkout pipelinespark\n",
    "- git checkout -b pipeline\n",
    "\n",
    "Luego agrega un poco de código para leer los parámetros del modelo desde una variable:\n",
    "Genial, el ingeniero B ha hecho una actualización que está comenzando a abstraer algunos de los parámetros. Luego empuja su nueva rama al repositorio remoto:\n",
    "- git push origin pipeline\n",
    "\n",
    "Finalmente, la ingeniera de ML C se une al equipo y quiere empezar a trabajar en el código. Al abrir Git y mirar las ramas, ve que hay tres:\n",
    "\n",
    "- main\n",
    "- pipeline1spark\n",
    "- pipeline\n",
    "\n",
    "Entonces, ¿cuál debería tomarse como el más actualizado? Si ella quiere hacer nuevas ediciones, ¿de dónde debería ramificarse? No está claro, pero más peligroso que eso es que si se le encarga lanzar código de implementación al entorno de ejecución, puede pensar que main tiene todos los cambios relevantes. En un proyecto mucho más concurrido que ha estado en marcha durante un tiempo, ¡incluso podría ramificarse desde main y duplicar parte del trabajo de B y C! En un proyecto pequeño, perderías tiempo persiguiendo sombras; en un proyecto grande con muchas líneas de trabajo diferentes, tendrías muy poca oportunidad de mantener un buen flujo de trabajo.\n",
    "\n",
    "\" Branch pipeline1spark - Commit 1 (Engineer A)\"\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\" Branch pipeline - Commit 2 (Engineer B)\"\n",
    "lr = LogisticRegression(maxIter=model_config[\"maxIter\"],\n",
    "regParam=model_config[\"regParam\"])\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "Si estos cambios se envían a la rama principal al mismo tiempo, entonces obtendremos lo que se llama un conflicto de fusión, y en cada caso el ingeniero tendrá que elegir qué parte del código mantener, el ejemplo actual o el nuevo. Esto se vería algo así si el ingeniero A envió sus cambios a la rama principal primero:\n",
    "\n",
    "NOTA IMPORTANTE: `Aunque en este caso simple podríamos confiar en que los ingenieros seleccionen el mejor código, permitir que situaciones como esta ocurran con mucha frecuencia es un gran riesgo para su proyecto. Esto no solo desperdicia una gran cantidad de tiempo de desarrollo valioso, sino que también podría significar que en realidad termines con un código peor!`\n",
    "\n",
    "La manera de evitar confusiones y trabajo extra como este es tener una estrategia muy clara para el uso de su sistema de control de versiones, como la que ahora exploraremos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fd24f",
   "metadata": {},
   "source": [
    "### Gitflow Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2546ebd",
   "metadata": {},
   "source": [
    "El mayor problema con el ejemplo anterior era que todos nuestros ingenieros hipotéticos estaban trabajando en el mismo fragmento de código en diferentes lugares. Para detener situaciones como esta, tienes que crear un proceso que tu equipo pueda seguir – en otras palabras, una estrategia de control de versiones o flujo de trabajo.\n",
    "\n",
    "La idea principal es que tenemos los siguientes tipos de ramas:\n",
    "\n",
    "- Main\n",
    "- Dev\n",
    "- Release\n",
    "- Feature\n",
    "- Hotfix\n",
    "\n",
    "La creación de cada uno es por una razón clara y extremadamente específica. Algunas pautas generales para comenzar con esta metodología son las siguientes:\n",
    "\n",
    "- `Main`: contiene tus lanzamientos oficiales y solo debe contener la versión estable de tu código.\n",
    "- `Dev` : actúa como el punto principal para ramificarse y fusionarse para la mayoría del trabajo en el repositorio; Contiene el desarrollo continuo de la base de código y actúa como un área de puesta en escena antes de Main.\n",
    "- `Feature`: Las ramas no deben ser fusionadas directamente en la rama principal; todo debe ramificarse desde dev y luego fusionarse de nuevo en dev.\n",
    "- `Release`: Las ramas de lanzamiento se crean a partir de dev para iniciar un proceso de construcción o liberación, antes de ser fusionadas en main y dev y luego eliminadas.\n",
    "- `Hosfix`: Las ramas de hotfix son para eliminar errores en software desplegado o en producción. Puedes crear una rama de esto desde la rama principal antes de fusionar en la principal y en desarrollo cuando hayas terminado.\n",
    "\n",
    "Si tu proyecto de ML puede seguir este tipo de estrategia (y no necesitas ser completamente estricto al respecto si deseas adaptarlo), es probable que veas una mejora drástica en la productividad, la calidad del código e incluso la documentación.\n",
    "\n",
    "Un aspecto importante que no hemos discutido aún es el concepto de revisiones de código. Estas se desencadenan en este proceso mediante lo que se conoce como una solicitud de extracción (pull request), donde haces saber tu intención de fusionar en otra rama y permites que otro miembro del equipo revise tu código antes de que esto se ejecute. Esta es la forma natural de introducir la revisión de código en tu flujo de trabajo. Haces esto cada vez que deseas fusionar tus cambios y actualizarlos en las ramas dev o main. Los cambios propuestos pueden ser visibles para el resto del equipo, donde pueden ser debatidos e iterados con más commits antes de completar la fusión. Esto refuerza la revisión de código para mejorar la calidad, así como crear un historial de auditoría y salvaguardias para las actualizaciones. \n",
    "\n",
    "Ahora que hemos discutido algunas de las mejores prácticas para aplicar el control de versiones a tu código, exploremos cómo controlar la versión de los modelos que produces durante tu proyecto de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26d848",
   "metadata": {},
   "source": [
    "### Control de versiones del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e1967",
   "metadata": {},
   "source": [
    "En cualquier proyecto de ingeniería de ML, no solo tienes que rastrear claramente los cambios en el código, sino también los cambios en tus modelos. Quieres rastrear cambios no solo en el enfoque de modelado, sino también en el rendimiento cuando se alimentan datos nuevos o diferentes a tus algoritmos elegidos. Una de las mejores herramientas para rastrear este tipo de cambios y proporcionar control de versiones de los modelos de ML es MLflow, que instalamos anteriormente en este capítulo.\n",
    "\n",
    "El objetivo principal de MLflow es proporcionar una plataforma a través de la cual puedes registrar experimentos de modelos, artefactos y métricas de rendimiento. Esto se realiza a través de unas APIs muy simples proporcionadas por la biblioteca mlflow de Python, que se interfazan con soluciones de almacenamiento seleccionadas a través de una serie de plugins desarrollados de manera central y por la comunidad. También incluye funcionalidades para consultar, analizar e importar/exportar datos a través de una Interfaz Gráfica de Usuario (GUI).\n",
    "\n",
    "La biblioteca es extremadamente fácil de usar. En el siguiente ejemplo, tomaremos el ejemplo de pronóstico de ventas del Capítulo 1, Introducción a la Ingeniería de ML, y agregaremos algunas funcionalidades básicas de MLflow para rastrear métricas de rendimiento y guardar el modelo de Prophet entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d169ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import mlflow\n",
    "import mlflow.pyfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c1971",
   "metadata": {},
   "source": [
    "Definamos la ssiguiente clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6ac309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Series_TF/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "class FbProphetWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self):\n",
    "        self.model = model\n",
    "        super().__init__()\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        from prophet import Prophet\n",
    "        return\n",
    "    def predict(self, context, model_input):\n",
    "        future = self.model.make_future_dataframe(periods=model_input[\"periods\"][0])\n",
    "        return self.model.predict(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ccc4e3",
   "metadata": {},
   "source": [
    "Ahora agrupamos la funcionalidad para el entrenamiento y la predicción en una única función auxiliar llamada train_predict() para facilitar la ejecución varias veces. No definiremos todos los detalles dentro de esta función aquí, pero repasemos las piezas principales de la funcionalidad de MLflow que contiene.\n",
    "\n",
    "1. usar nuestra clase envoltura de modelo para registrar el modelo e imprimir alguna información sobre la ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    model = Prophet(\n",
    "    yearly_seasonality=seasonality_params['yearly'],\n",
    "    weekly_seasonality=seasonality_params['weekly'],\n",
    "    daily_seasonality=seasonality_params['daily']\n",
    "    )\n",
    "    model.fit(df_train)\n",
    "    df_cv = cross_validation(model, initial=\"730 days\", period=\"180 days\", horizon=\"365 days\")\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    mlflow.log_metric(\"rmse\", df_p.loc[0, \"rmse\"])\n",
    "    mlflow.pyfunc.log_model(\"model\", python_model=FbProphetWrapper(model)) #1\n",
    "    print(\n",
    "    \"Logged model with URI: runs:/{run_id}/model\".format(\n",
    "    run_id=mlflow.active_run().info.run_id\n",
    "    )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82223ac3",
   "metadata": {},
   "source": [
    "¡Con sólo unas pocas líneas adicionales, hemos comenzado a realizar control de versiones en nuestros modelos y a seguir las estadísticas de diferentes ejecuciones!\n",
    "\n",
    "Existen muchas formas diferentes de guardar el modelo de ML que has construido en MLflow (y en general), lo cual es especialmente importante al rastrear las versiones del modelo. Algunas de las principales opciones son las siguientes:\n",
    "\n",
    "- `pickle` : pickle es una biblioteca de Python para la serialización de objetos que a menudo se utiliza para la exportación de modelos de ML que están escritos en scikit-learn o flujos de trabajo en el ecosistema más amplio de scipy (https://docs.python.org/3/library/pickle.html#module-pickle). Aunque es extremadamente fácil de usar, debes tener cuidado al exportar tus modelos a archivos pickle debido a lo siguiente:\n",
    "    - Versionado: Cuando haces un pickle de un objeto, tienes que deserializarlo en otros programas utilizando la misma versión de pickle por razones de estabilidad. Esto añade más complejidad a la gestión de tu proyecto.\n",
    "    - Seguridad: La documentación de pickle indica claramente que no es seguro y que es muy fácil construir pickles maliciosos que ejecutarán código peligroso al deserializarlos. Esta es una consideración muy importante, especialmente a medida que te acercas a la producción.\n",
    "- `joblib`: joblib es una biblioteca de canalización de propósito general en Python que es muy poderosa pero liviana. Tiene muchas capacidades realmente útiles centradas en el almacenamiento en caché, la paralelización y la compresión que la convierten en una herramienta muy versátil para guardar y leer en tus pipelines de aprendizaje automático. Usaremos joblib más en capítulos posteriores.\n",
    "- `JSON`: Si pickle y joblib no son apropiados, puedes serializar tu modelo y sus parámetros en formato JSON. Esto es bueno porque JSON es un formato de serialización de texto estandarizado que se utiliza comúnmente en una variedad de soluciones y plataformas. La advertencia de usar la serialización JSON de tus modelos es que a menudo tienes que definir manualmente la estructura JSON con los parámetros relevantes que deseas almacenar. Así que puede generar mucho trabajo adicional. Varias bibliotecas de ML en Python tienen su propia funcionalidad de exportación a JSON, por ejemplo, el paquete de aprendizaje profundo Keras, pero todos pueden resultar en formatos bastante diferentes.\n",
    "- `MLeap`: MLeap es un formato de serialización y un motor de ejecución basado en la Máquina Virtual de Java (JVM). Tiene integraciones con Scala, PySpark y scikit-learn, pero a menudo lo verás utilizado en ejemplos y tutoriales para guardar pipelines de Spark, especialmente para modelos construidos con Spark MLlib. Este enfoque significa que no es el formato más flexible, pero es muy útil si trabajas en el ecosistema de Spark.\n",
    "- `ONNX`: El formato Open Neural Network Exchange (ONNX) está diseñado para ser completamente multiplataforma y permitir el intercambio de modelos entre los principales marcos y ecosistemas de aprendizaje automático. La principal desventaja de ONNX es que (como puedes adivinar por el nombre) está dirigido principalmente a modelos basados en redes neuronales, con la excepción de su API de scikit-learn. Sin embargo, es una opción excelente si estás construyendo una red neuronal.\n",
    "\n",
    "En el Capítulo 3, De modelo a fábrica de modelos, exportaremos nuestros modelos a MLflow utilizando algunos de estos formatos, pero todos son compatibles con MLfl y, por lo tanto, deberías sentirte cómodo utilizándolos como parte de tu flujo de trabajo de ingeniería de ML. La sección final de este capítulo presentará algunos conceptos importantes para planificar cómo deseas implementar tu solución, precediendo discusiones más detalladas más adelante en el libro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5139a17",
   "metadata": {},
   "source": [
    "## Desplegar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f38f2",
   "metadata": {},
   "source": [
    "La etapa final del proceso de desarrollo de ML es la que realmente importa: ¿cómo llevar la increíble solución que has construido al mundo real y resolver tu problema original? La respuesta tiene múltiples partes, algunas de las cuales nos ocuparán más a fondo más adelante en este libro, pero serán delineadas en esta sección. Si vamos a desplegar nuestra solución con éxito, primero que nada, necesitamos conocer nuestras opciones de despliegue: ¿qué infraestructura está disponible y es apropiada para la tarea? Luego necesitamos llevar la solución de nuestro entorno de desarrollo a esta infraestructura de producción para que, sujeto a la orquestación y controles apropiados, pueda ejecutar las tareas que necesitamos que realice y presentar los resultados donde sea necesario. Aquí es donde entran en juego los conceptos de DevOps y MLOps.\n",
    "\n",
    "Elaboremos sobre estos dos conceptos centrales, sentando las bases para los capítulos posteriores y explorando cómo comenzar a implementar nuestro trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39869b85",
   "metadata": {},
   "source": [
    "### Conocer sus opciones de implementacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df158cb6",
   "metadata": {},
   "source": [
    "En el Capítulo 5, Patrones y Herramientas de Despliegue, cubriremos en detalle qué necesitas para llevar tu proyecto de ingeniería de ML de la etapa de desarrollo a la de despliegue, pero para anticipar eso y proporcionar un adelanto de lo que está por venir, exploremos los diferentes tipos de opciones de despliegue que tenemos a nuestra disposición:\n",
    "\n",
    "- `On-premises deployment` : La primera opción que tenemos es ignorar completamente la nube pública y desplegar nuestras soluciones internamente en infraestructura propia. Esta opción es particularmente popular y necesaria para muchas instituciones grandes con mucho software heredado y fuertes restricciones regulatorias sobre la ubicación y el procesamiento de datos. Los pasos básicos para implementar en las instalaciones son los mismos que para implementar en la nube, pero a menudo requieren mucha más participación de otros equipos con especialidades particulares. Por ejemplo, si estás en la nube, a menudo no necesitas pasar mucho tiempo configurando redes o implementando equilibradores de carga, mientras que las soluciones en las instalaciones requerirán estas. La gran ventaja del despliegue on-premises es la seguridad y la tranquilidad de que ninguno de tus datos va a atravesar el cortafuegos de tu empresa. Las grandes desventajas son que requiere una inversión mayor por adelantado para hardware y que tienes que dedicar mucho esfuerzo para configurar y gestionar ese hardware de manera efectiva. No discutiremos el despliegue on-premises en detalle en este libro, pero todos los conceptos que utilizaremos sobre el desarrollo de software, empaquetado, gestión de entornos y sistemas de entrenamiento y predicción aún son aplicables.\n",
    "- `Infrastructure-as-a-Service (IaaS)`: Si va a utilizar la nube, uno de los niveles más bajos de abstracción a los que tiene acceso para la implementación son las soluciones IaaS. Típicamente se basan en el concepto de virtualización, de modo que los servidores con una variedad de especificaciones se pueden aumentar a la voluntad del usuario. Estas soluciones a menudo abstractan la necesidad de mantenimiento y operaciones como parte del servicio. Lo más importante es que permiten la escalabilidad extrema de su infraestructura tal como la necesita. ¿Tienes que ejecutar 100 servidores más la próxima semana? No hay problema, solo amplíe su solicitud de IaaS y allí está. Aunque las soluciones de IaaS son un gran paso más de la infraestructura loca totalmente administrada, todavía hay varias cosas en las que debe pensar y configurar. El equilibrio en\n",
    "La computación en la nube siempre se trata de lo fácil que desea que sean las cosas en comparación con el nivel de control que desea tener. IaaS maximiza el control pero minimiza (relativa) la facilidad en comparación con algunas otras soluciones. En AWS, el servicio de almacenamiento simple (S3) y la nube de cómputo elástica (EC2) son buenos ejemplos de ofertas de IaaS.\n",
    "- `Platform-as-a-Service (PaaS)`: Las soluciones PaaS son el siguiente nivel en términos de abstracción y generalmente te ofrecen muchas capacidades sin necesidad de saber exactamente lo que sucede detrás de escena. Esto significa que puedes concentrarte únicamente en las tareas de desarrollo que la plataforma está diseñada para soportar, sin preocuparte por la infraestructura subyacente en absoluto. Un buen ejemplo son las funciones de AWS Lambda, que son funciones sin servidor que pueden escalar casi sin límite. Todo lo que se requiere de ti es ingresar la pieza principal de código que deseas ejecutar dentro de la función. Otro buen ejemplo es Databricks, que proporciona una interfaz de usuario muy intuitiva sobre la infraestructura del clúster Spark, con la capacidad de aprovisionar, configurar y escalar estos clústeres casi sin problemas. \n",
    "\n",
    "\n",
    "Ser consciente de estas diferentes opciones y sus capacidades puede ayudarte a diseñar tu solución de aprendizaje automático y asegurarte de que enfoques el esfuerzo de ingeniería de tu equipo donde más se necesita y donde será más valioso. Si tu ingeniero de aprendizaje automático está trabajando en la configuración de enrutadores, por ejemplo, definitivamente has cometido un error en algún lugar.\n",
    "\n",
    "Pero una vez que hayas seleccionado los componentes que utilizarás y provisionado la infraestructura, ¿cómo los integras y gestionas tus ciclos de implementación y actualización? Esto es lo que exploraremos ahora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd18ccfe",
   "metadata": {},
   "source": [
    "### DevOps y MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276403b5",
   "metadata": {},
   "source": [
    "Una idea muy poderosa en el desarrollo de software moderno es que tu equipo debería ser capaz de actualizar continuamente tu base de código según sea necesario, mientras que las pruebas, la integración, la construcción, el empaquetado y la implementación de tu solución deberían ser lo más automatizadas posible. Esto significa que estos procesos pueden ocurrir de manera casi continua sin que se asignen grandes bloques de tiempo planificados para los ciclos de actualización. Esta es la idea principal detrás de CI/CD. CI/CD es una parte esencial de DevOps y su primo enfocado en ML, MLOps, que ambos tienen como objetivo unir el desarrollo de software y las operaciones posteriores a la implementación. Varios de los conceptos y soluciones que desarrollaremos en este libro estarán construidos de tal manera que se ajusten naturalmente dentro de un marco de MLOps.\n",
    "\n",
    "La parte de CI se centra principalmente en la incorporación estable de los cambios en curso en la base de código, asegurando que la funcionalidad permanezca estable. La parte de CD se trata de tomar la versión estable resultante de la solución y llevarla a la infraestructura apropiada. \n",
    "\n",
    "Para hacer de la CI/CD una realidad, necesitas incorporar herramientas que ayuden a automatizar tareas que tradicionalmente realizarías manualmente en tu proceso de desarrollo y despliegue. Por ejemplo, si puedes automatizar la ejecución de pruebas al fusionar código, o el envío de tus artefactos/modelos de código al entorno apropiado, entonces estás en buen camino hacia la CI/CD.\n",
    "\n",
    "Usaremos GitHub Actions como nuestra herramienta de CI/CD en este libro, pero hay varias otras herramientas disponibles que hacen el mismo trabajo.\n",
    "\n",
    "Al utilizar GitHub Actions, debes crear un archivo .yml que indique a GitHub cuándo realizar las acciones necesarias y, por supuesto, qué acciones realizar. En el siguiente código, construiremos un ejemplo de archivo .yml para un proyecto de Python donde automáticamente se instalan las dependencias, se ejecuta un linter (una solución para verificar errores, errores de sintaxis y otros problemas), y luego se ejecutan algunas pruebas unitarias. Este ejemplo proviene del repositorio de GitHub Starter Workflows (https://github.com/actions/starter-workflows/blob/main/ci/python-package-conda.yml):\n",
    "\n",
    "1. Primero, defines el nombre del flujo de trabajo de GitHub Actions y qué evento de Git lo activará:\n",
    "    - name: Python package\n",
    "    - on: [push]\n",
    "2. Luego enumeras los trabajos que deseas ejecutar como parte del flujo de trabajo, así como su configuración. Por ejemplo, aquí tenemos un trabajo llamado build, que queremos ejecutar en la última distribución de Ubuntu, y queremos intentar la construcción utilizando varias versiones diferentes de Python:\n",
    "\n",
    "jobs:\n",
    "\n",
    "    build:\n",
    "\n",
    "        runs-on: ubuntu-latest\n",
    "        strategy:\n",
    "\n",
    "            matrix:\n",
    "\n",
    "                python-version: [2.7, 3.5, 3.6, 3.7, 3.8]\n",
    "\n",
    "3. Luego defines los pasos que se ejecutan como parte del trabajo. Cada paso está separado por un guion y se ejecuta como un comando separado. Es importante señalar que la palabra clave que uses capture acciones estándar de GitHub; por ejemplo, en el primer paso, el flujo de trabajo utiliza la versión v2 de la acción de checkout y el segundo paso configura las versiones de Python que queremos ejecutar en el flujo de trabajo:\n",
    "\n",
    "workflow:\n",
    "\n",
    "    steps:\n",
    "\n",
    "        - uses: actions/checkout@v2\n",
    "        - name: Set up Python ${{ matrix.python-version }}\n",
    "        uses: actions/setup-python@v2\n",
    "\n",
    "        with:\n",
    "\n",
    "            python-version: ${{ matrix.python-version }}\n",
    "\n",
    "4. El siguiente paso instala las dependencias relevantes para la solución utilizando pip y un archivo requirements.txt (¡pero puedes usar conda, por supuesto!):\n",
    "\n",
    "- name: Install dependencies\n",
    "    run: |\n",
    "    python -m pip install --upgrade pip\n",
    "    pip install flake8 pytest\n",
    "    if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n",
    "\n",
    "5. Luego ejecutamos un poco de linting:\n",
    "\n",
    "- name: Lint with flake8\n",
    "\n",
    "    run: |\n",
    "\n",
    "    \"# stop the build if there are Python syntax errors or undefined names\"\n",
    "\n",
    "    flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n",
    "\n",
    "    \"# exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\"\n",
    "\n",
    "    flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n",
    "\n",
    "6. Finalmente, ejecutamos nuestras pruebas utilizando nuestra biblioteca de pruebas favorita de Python:\n",
    "\n",
    "- name: Test with pytest\n",
    "\n",
    "    run: |\n",
    "\n",
    "    pytest\n",
    "\n",
    "Y eso, en resumen, es cómo comienzas a construir tus pipelines de CI/CD. Más adelante en el libro, construiremos flujos de trabajo específicos para nuestras soluciones de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfccf8",
   "metadata": {},
   "source": [
    "# Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd20688",
   "metadata": {},
   "source": [
    "Este capítulo se trató de construir una base sólida para el trabajo futuro. Discutimos las etapas de desarrollo comunes a todos los proyectos de ingeniería de ML, que denominamos descubrir, jugar, desarrollar, desplegar. En particular, esbozamos el propósito de cada una de estas etapas y sus resultados deseados. Esto fue seguido de una discusión a alto nivel sobre las herramientas, y una guía sobre los principales pasos de configuración. \n",
    "\n",
    "Configuramos las herramientas para desarrollar nuestro código, haciendo un seguimiento de los cambios de ese código, gestionando nuestro proyecto de ingeniería de ML y, finalmente, desplegando nuestras soluciones.\n",
    "\n",
    "En el resto del capítulo, revisamos los detalles de cada una de las cuatro etapas que describimos previamente, con un enfoque particular en las etapas de desarrollo y despliegue. Nuestra discusión abarcó todo, desde los pros y los contras de las metodologías de desarrollo Waterfall y Agile, hasta la gestión del entorno y las mejores prácticas en desarrollo de software. También discutimos cómo aplicar pruebas a nuestro código de ML. Concluimos con una exploración de cómo empaquetar tu solución de ML y qué infraestructura de despliegue está disponible para que la uses, y esbozamos los conceptos básicos para establecer tus flujos de trabajo de DevOps y MLOps.\n",
    "\n",
    "En el próximo capítulo, dirigiremos nuestra atención a cómo desarrollar el software para realizar el entrenamiento y reentrenamiento automatizado de tus modelos utilizando muchas de las técnicas que hemos estado discutiendo aquí.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Series_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
