{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb2e195",
   "metadata": {},
   "source": [
    "**Tabla de contenido**\n",
    "\n",
    "- [Requisitos técnicos](#Requisitos-tecnicos)\n",
    "- [Definiendo la fábrica de modelos](#Definiendo-la-fabrica-de-modelos)\n",
    "- [Diseñando tu sistema de entrenamiento](#Disenando-tu-sistema-de-entrenamiento)\n",
    "    - [Opciones de diseño del sistema de entrenamiento](#Opciones-de-diseno-del-sistema-de-entrenamiento)\n",
    "    - [Train-run](#Train-run)\n",
    "    - [Train-persist](#Train-persist)\n",
    "    - [Retraining required](#Retraining-required)\n",
    "    - [Detectando el drift](#Detectando-el-drift)\n",
    "- [Ingeniería de características para el consumo](#Ingenieria-de-caracteristicas-para-el-consumo)\n",
    "    - [Engineering categorical features](#Engineering-categorical-features)\n",
    "    - [Engineering numerical features](#Engineering-numerical-features)\n",
    "    - [Definiendo el objetivo](#Definiendo-el-objetivo)\n",
    "    - [Reducir tus pérdidas](#Reducir-tus-perdidas)\n",
    "    - [Jerarquías de automatización](#Jerarquias-de-automatizacion)\n",
    "- [Optimizing hyperparameters](#Optimizing-hyperparameters)\n",
    "- [Auto-sklearn](#Auto-sklearn)\n",
    "- [Optuna en redes neuronales](#Optuna-en-redes-neuronales)\n",
    "- [Persisting your models](#Persisting-your-models)\n",
    "- [Construyendo la fábrica de modelos con pipelines](#Construyendo-la-fabrica-de-modelos-con-pipelines)\n",
    "    - [Scikit-learn pipelines](#Scikit-learn-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f8db21",
   "metadata": {},
   "source": [
    "Este capítulo trata sobre uno de los conceptos más importantes en la ingeniería de ML: ¿cómo puedes tomar la difícil tarea de entrenar y ajustar tus modelos y convertirla en algo que puedas automatizar, reproducir y escalar para sistemas de producción?\n",
    "\n",
    "Recapitularemos las ideas principales detrás del entrenamiento de diferentes modelos de ML a nivel teórico y práctico, antes de proporcionar motivación para el reentrenamiento, a saber, la idea de que los modelos de ML no rendirán bien para siempre. Este concepto también se conoce como drift. A continuación, cubriremos algunos de los conceptos principales detrás de la ingeniería de características, que es una parte clave de cualquier tarea de ML. A continuación, profundizaremos en cómo funciona el ML y cómo es, en esencia, una serie de problemas de optimización. Exploraremos cómo, al abordar estos problemas de optimización, puedes hacerlo con una variedad de herramientas en diferentes niveles de abstracción. En particular, discutiremos cómo puedes proporcionar la definición directa del modelo que deseas entrenar, a lo que llamo 'manualmente', o cómo puedes realizar la optimización de hiperparámetros o Aprendizaje Automático Automatizado (AutoML). Veremos ejemplos de uso de diferentes bibliotecas y herramientas que hacen todo esto, antes de explorar cómo implementarlas para su uso posterior en tu flujo de trabajo de entrenamiento.\n",
    "\n",
    "A continuación, construiremos sobre el trabajo introductorio que hicimos en el Capítulo 2, El Proceso de Desarrollo de Aprendizaje Automático, sobre MLflow, mostrándote cómo interactuar con las diferentes API de MLflow para gestionar tus modelos y actualizar sus estados en el Registro de Modelos de MLflow.\n",
    "\n",
    "Terminará este capítulo discutiendo las utilidades que le permiten encadenar todos sus pasos de entrenamiento de modelos de ML en unidades individuales conocidas como pipelines, que pueden ayudar a actuar como representaciones más compactas de todos los pasos que hemos discutido anteriormente. El resumen al final recapitulará los mensajes clave y también señalará cómo lo que hemos hecho aquí se desarrollará más en el Capítulo 4, Empaquetado, y el Capítulo 5, Patrones y Herramientas de Despliegue.\n",
    "\n",
    "En esencia, este capítulo te dirá lo que necesitas para unir en tu solución, mientras que los capítulos posteriores te dirán cómo unirlos de manera robusta. Cubriremos esto en las siguientes secciones:\n",
    "\n",
    "- Defining the model factory\n",
    "- Designing your training system\n",
    "- Retraining required\n",
    "- Learning about learning\n",
    "- Persisting your models\n",
    "- Building the model factory with pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8245a",
   "metadata": {},
   "source": [
    "# Requisitos tecnicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad92cd6",
   "metadata": {},
   "source": [
    "Para completar este capítulo, necesitarás haber instalado los siguientes paquetes y herramientas de Python.\n",
    "\n",
    "- MLflow: gestión del ciclo de vida de los modelos.\n",
    "- TensorFlow\n",
    "- auto-keras\n",
    "- Hyperopt\n",
    "- Optuna\n",
    "- auto-sklearn\n",
    "- alibi-detect: monitoreo de modelos en producción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b8425",
   "metadata": {},
   "source": [
    "# Definiendo la fabrica de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4be5b",
   "metadata": {},
   "source": [
    "Si queremos desarrollar soluciones que se alejen de una ejecución ad hoc, manual e inconsistente y se dirijan hacia sistemas de aprendizaje automático que puedan ser automatizados, robustos y escalables, entonces debemos abordar la cuestión de cómo crearemos y curaremos la estrella del espectáculo: los propios modelos.\n",
    "\n",
    "En este capítulo, discutiremos los componentes clave que deben ser reunidos para avanzar hacia esta visión y proporcionaremos algunos ejemplos de cómo pueden verse en código. Estos ejemplos no son la única forma de implementar estos conceptos, pero nos permitirán comenzar a construir nuestras soluciones de ML hacia el nivel de sofisticación que necesitaremos si queremos desplegar en el mundo real.\n",
    "\n",
    "Los principales componentes de los que estamos hablando aquí son los siguientes:\n",
    "\n",
    "- `Training system`: Un sistema para entrenar de manera robusta nuestros modelos con los datos que tenemos de forma automatizada. Esto consiste en todo el código que hemos desarrollado para entrenar nuestros modelos de aprendizaje automático con los datos.\n",
    "- `Model Store`: Un lugar para persistir modelos entrenados con éxito y un lugar para compartir modelos listos para producción con componentes que ejecutarán las predicciones.\n",
    "\n",
    "- `Drift detector`: un sistema para detectar cambios en el rendimiento del modelo que desencadenen ejecuciones de entrenamiento.\n",
    "\n",
    "Estos componentes, combinados con su interacción con el sistema de predicción implementado, abarcan la idea de una fábrica de modelos.\n",
    "\n",
    "En el resto de este capítulo, exploraremos en detalle los tres componentes que mencionamos anteriormente. Los sistemas de predicción serán el enfoque de capítulos posteriores, especialmente el Capítulo 5, Patrones y Herramientas de Despliegue. Primero, exploremos qué significa entrenar un modelo de ML y cómo podemos construir sistemas para hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ad592",
   "metadata": {},
   "source": [
    "# Disenando tu sistema de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e42ec8",
   "metadata": {},
   "source": [
    "Visto en el nivel más alto, los modelos de ML pasan por un ciclo de vida con dos etapas: una fase de entrenamiento y una fase de salida. Durante la fase de entrenamiento, se alimenta al modelo con datos para aprender del conjunto de datos. En la fase de predicción, el modelo, completo con sus parámetros optimizados, recibe nuevos datos en orden y devuelve la salida deseada.\n",
    "\n",
    "Estas dos fases tienen requisitos computacionales y de procesamiento muy diferentes. En la fase de entrenamiento, tenemos que exponer al modelo a la mayor cantidad de datos posible para obtener el mejor rendimiento, todo mientras aseguramos que se reserve un subconjunto de datos para pruebas y validación. El entrenamiento del modelo es fundamentalmente un problema de optimización, que requiere varios pasos incrementales para llegar a una solución. Por lo tanto, esto es computacionalmente exigente, y en casos donde los datos son relativamente grandes (o los recursos computacionales son relativamente bajos), puede tomar mucho tiempo.\n",
    "\n",
    "Incluso si tuvieras un conjunto de datos pequeño y muchos recursos computacionales, el entrenamiento todavía no es un proceso de baja latencia. Además, es un proceso que a menudo se ejecuta en lotes y donde pequeñas adiciones al conjunto de datos no harán tanta diferencia en el rendimiento del modelo (hay excepciones a esto). La predicción, por otro lado, es un proceso más simple y se puede pensar de la misma manera que ejecutar cualquier cálculo o función en tu código: las entradas entran, se realiza un cálculo y el resultado sale. Esto (en general) no es computacionalmente exigente y tiene baja latencia.\n",
    "\n",
    "Tomando esto en conjunto, significa que, en primer lugar, tiene sentido separar estos dos pasos (entrenamiento y predicción) tanto lógicamente como en el código. En segundo lugar, significa que tenemos que considerar los diferentes requisitos de ejecución para estas dos etapas y construyéndolo en nuestros diseños de soluciones. Finalmente, necesitamos tomar decisiones sobre nuestro régimen de entrenamiento, incluyendo si programamos el entrenamiento en lotes, usamos aprendizaje incremental, o si debemos activar el entrenamiento basado en criterios de rendimiento del modelo. Estas son las partes clave de tu sistema de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaaf43e",
   "metadata": {},
   "source": [
    "## Opciones de diseno del sistema de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41cec2",
   "metadata": {},
   "source": [
    "Antes de crear cualquier diseño detallado de nuestro sistema de entrenamiento, siempre se aplicarán algunas preguntas generales.\n",
    "\n",
    "- ¿Hay infraestructura disponible que sea apropiada para el problema? \n",
    "- ¿Dónde están los datos y cómo los alimentaremos al algoritmo? \n",
    "- ¿Cómo estoy probando el rendimiento del modelo?\n",
    "\n",
    "En términos de infraestructura, esto puede depender mucho del modelo y los datos que estés utilizando para el entrenamiento. Si vas a entrenar una regresión lineal con datos que tienen tres características y tu conjunto de datos contiene solo 10,000 registros tabulares, es probable que puedas ejecutar esto en hardware de escala de portátil sin pensarlo demasiado. No son muchos datos, y tu modelo no tiene muchos parámetros libres. Si estás entrenando en un conjunto de datos mucho más grande, como uno que contiene 100 millones de registros tabulares, entonces podrías beneficiarte de la paralelización a través de algo como un clúster de Spark. Si, sin embargo, estás entrenando una red neuronal convolucional profunda de 100 capas con 1,000 imágenes, entonces es probable que desees usar una GPU. Hay muchas opciones, pero la clave es elegir lo correcto para el trabajo.\n",
    "\n",
    "Con respecto a la cuestión de cómo alimentamos datos al algoritmo, esto puede ser no trivial. ¿Vamos a ejecutar una consulta SQL contra una base de datos alojada de forma remota? Si es así, ¿cómo nos estamos conectando a ella? ¿La máquina en la que estamos ejecutando la consulta tiene suficiente RAM para almacenar los datos? Si no, ¿necesitamos considerar el uso de un algoritmo que pueda aprender de manera incremental? Para las pruebas de rendimiento algorítmico clásico, necesitamos emplear los trucos bien conocidos del comercio de ML y realizar divisiones de entrenamiento/prueba/validación en nuestros datos. También necesitamos decidir qué estrategias de validación cruzada queremos emplear. Luego necesitamos seleccionar nuestra métrica de rendimiento del modelo y calcularla adecuadamente. Sin embargo, como ingenieros de ML, también estaremos interesados en otras medidas de rendimiento, como el tiempo de entrenamiento, el uso eficiente de la memoria, la latencia y (me atrevo a decirlo) el costo. Necesitaremos entender cómo podemos medir y luego optimizar estos también.\n",
    "\n",
    "Siempre y cuando tengamos en cuenta estas cosas a medida que avancemos, estaremos en una buena posición. Ahora, pasemos al diseño. Como mencionamos en la introducción a esta sección, tenemos dos piezas fundamentales a considerar: los procesos de entrenamiento y output. Hay dos formas en las que podemos juntar esto para nuestra solución. Discutiremos esto en la próxima sección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7b7e9",
   "metadata": {},
   "source": [
    "## Train-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6112fc",
   "metadata": {},
   "source": [
    "La opción 1 consiste en realizar el entrenamiento y la predicción en el mismo proceso, con el entrenamiento ocurriendo en modo por lotes o incremental. Este patrón se llama entrenar-ejecutar.\n",
    "\n",
    "Este patrón es el más simple de los dos, pero también el menos deseable para problemas del mundo real, ya que no encarna el principio de separación de preocupaciones que mencionamos anteriormente. Esto no significa que sea un patrón inválido, y tiene la ventaja de ser a menudo más simple de implementar. Aquí, ejecutamos todo nuestro proceso de entrenamiento antes de hacer nuestras predicciones, sin una pausa real entre ellos. Dadas nuestras discusiones anteriores, podemos descartar automáticamente este enfoque si tenemos que servir predicciones de manera muy baja en latencia; por ejemplo, a través de una solución basada en eventos o por streaming (más sobre esto más adelante).\n",
    "\n",
    "Donde este enfoque podría ser completamente válido, aunque (y he visto esto algunas veces en la práctica), es en casos donde los algoritmos que estás aplicando son realmente muy ligeros de entrenar y necesitas seguir usando datos muy recientes, o donde estás ejecutando un proceso por lotes grande relativamente infrecuentemente.\n",
    "\n",
    "Aunque este es un enfoque simple y no se aplica a todos los casos, tiene ventajas distintas:\n",
    "\n",
    "- Dado que te estás entrenando tan a menudo como predices, estás haciendo todo lo posible para protegerte contra la degradación del rendimiento moderno, lo que significa que estás combatiendo el driff (deslizamiento).\n",
    "\n",
    "- Estás reduciendo significativamente la complejidad de tu solución. Aunque estás acoplando estrechamente dos componentes, lo cual generalmente debería evitarse, las etapas de entrenamiento y predicción pueden ser tan simples de codificar que si simplemente las unes, ahorrarás mucho tiempo de desarrollo. Este es un punto no trivial porque el tiempo de desarrollo cuesta dinero.\n",
    "\n",
    "Ahora, veamos el otro caso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4ed1f",
   "metadata": {},
   "source": [
    "## Train-persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7df4f1",
   "metadata": {},
   "source": [
    "La opción 2 es que el entrenamiento se realice en lote, mientras que la predicción funcione en el modo que se considere apropiado, con la solución de predicción leyendo el modelo entrenado de un almacenamiento. Llamaremos a este patrón de diseño entrenar-persistir.\n",
    "\n",
    "Si vamos a entrenar nuestro modelo y luego a persistir el modelo para que pueda ser recogido más tarde por un proceso de predicción, entonces necesitamos asegurarnos de que algunas cosas estén en su lugar:\n",
    "\n",
    "- ¿Cuáles son nuestras opciones de almacenamiento de modelos?\n",
    "- ¿Hay un mecanismo claro para acceder a nuestro almacén de modelos (escribir y leer)?\n",
    "- ¿Con qué frecuencia deberíamos entrenar frente a con qué frecuencia predeciremos?\n",
    "\n",
    "En nuestro caso, resolveremos las dos primeras preguntas utilizando MLflow, que introdujimos en el Capítulo 2, El Proceso de Desarrollo de Aprendizaje Automático, pero que revisitaremos en secciones posteriores. También hay muchas otras soluciones disponibles. El punto clave es que, sin importar qué utilices como almacén de modelos y punto de entrega entre tus procesos de entrenamiento y predicción, debe ser utilizado de manera que sea robusto y accesible.\n",
    "\n",
    "El tercer punto es más complicado. Podrías decidir desde el principio que quieres entrenar en un horario y apegarte a eso. O podrías ser más sofisticado y desarrollar criterios de activación que deben cumplirse antes de que ocurra el entrenamiento. Nuevamente, esta es una elección que tú, como ingeniero de ML, necesitas hacer con tu equipo. Más adelante en este capítulo, discutiremos mecanismos para programar tus sesiones de entrenamiento.\n",
    "\n",
    "En la siguiente sección, exploraremos qué debes hacer si deseas activar tus ejecuciones de entrenamiento basándote en cómo el rendimiento de tu modelo podría estar degradándose con el tiempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c306ab",
   "metadata": {},
   "source": [
    "## Retraining required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820014cf",
   "metadata": {},
   "source": [
    "No esperarías que después de terminar tu educación, nunca más leas un artículo o un libro o hables con nadie, lo que significa que no podrías tomar decisiones informadas sobre lo que está sucediendo en el mundo. Así que no deberías esperar que un modelo de aprendizaje automático sea entrenado una vez y luego funcione bien para siempre.\n",
    "\n",
    "Esta idea es intuitiva, pero representa un problema formal para los modelos de ML conocido como drift. Drift es un término que abarca una variedad de razones por las cuales el rendimiento de su modelo disminuye con el tiempo. Se puede dividir en dos tipos principales:\n",
    "\n",
    "- `Concept drift`: Esto ocurre cuando hay un cambio en la relación fundamental entre las características de tus datos y el resultado que estás tratando de predecir. A veces, esto también se conoce como desviación de covariables. Un ejemplo podría ser que en el momento del entrenamiento, solo tienes una submuestra de datos que parece mostrar una relación lineal entre las características y tu resultado. Si resulta que, después de recopilar muchos más datos tras el despliegue, la relación es no lineal, entonces ha ocurrido una desviación conceptual. La mitigación contra esto es el reentrenamiento con datos que sean más representativos de la relación correcta.\n",
    "- `Data drift`: Esto sucede cuando hay un cambio en las propiedades estadísticas de las variables que estás utilizando como características. Por ejemplo, podrías estar usando la edad como una característica en uno de tus modelos, pero en el momento del entrenamiento, solo tienes datos de personas de 16 a 24 años. Si el modelo se despliega y tu sistema comienza a ingerir datos de una demografía de edad más amplia, entonces tienes desviación de datos.\n",
    "\n",
    "Detectar el desvío en tus modelos desplegados es una parte clave de MLOps y debe ser una prioridad en la mente de un ingeniero de ML. Si puedes construir tus sistemas de entrenamiento de tal manera que el reentrenamiento se active en función de una comprensión informada del desvío en tus modelos, ahorrarás muchos recursos computacionales al entrenar solo cuando sea necesario.\n",
    "\n",
    "La siguiente sección discutirá algunas de las maneras en que podemos detectar el cambio en nuestros modelos. Esto nos ayudará a comenzar a construir una estrategia de reentrenamiento inteligente en nuestra solución."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2f78d",
   "metadata": {},
   "source": [
    "## Detectando el drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7243f4",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos definido el drift y sabemos que detectarlo será importante si queremos construir sistemas de entrenamiento sofisticados. La siguiente pregunta lógica es: ¿cómo hacemos esto? Las definiciones de drift que dimos en la sección anterior eran muy cualitativas; podemos comenzar a hacer estas afirmaciones un poco más cuantitativas a medida que exploramos los cálculos y conceptos que pueden ayudarnos a detectar el drift.\n",
    "\n",
    "En esta sección, nos basaremos en gran medida en el paquete de Python alibi-detect de Seldon, que, al momento de escribir, no estaba disponible en Anaconda.org pero está disponible en PyPI. Para adquirir este paquete, utiliza los siguientes comandos:\n",
    "\n",
    "- pip install tensorflow[and-cuda]\n",
    "- pip install tensorflow-probability\n",
    "- pip install tf-keras\n",
    "- pip install alibi\n",
    "- pip install alibi-detect\n",
    "\n",
    "Es muy fácil de usar el paquete alibi-detect. En el siguiente ejemplo, trabajaremos con el conjunto de datos de vino de sklearn, que se utilizará en otras partes de este capítulo. En este primer ejemplo, dividiremos los datos 50/50 y llamaremos a un conjunto el conjunto de referencia y al otro el conjunto de prueba. Luego utilizaremos la prueba de Kolmogorov-Smirnov para demostrar que no ha habido drift de datos entre estos dos conjuntos de datos, como se esperaba, y luego agregaremos artificialmente algo de drift para mostrar que ha sido detectada con éxito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94949e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import alibi\n",
    "from alibi_detect.cd import TabularDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81308518",
   "metadata": {},
   "source": [
    "A continuación, debemos obtener y dividir los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data = load_wine()\n",
    "feature_names = wine_data.feature_names\n",
    "X, y = wine_data.data, wine_data.target\n",
    "X_ref, X_test, y_ref, y_test = train_test_split(X, y,test_size=0.50,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497bad6",
   "metadata": {},
   "source": [
    "A continuación, debemos inicializar nuestro detector de drift utilizando los datos de referencia y proporcionando el valor p que queremos que se utilice en las pruebas de significancia estadística. Si deseas que tu detector de drift se active cuando ocurran diferencias más pequeñas en la distribución de los datos, debes seleccionar un valor p más grande.\n",
    "\n",
    "Cuando usas TabularDrift, el detector necesita saber si cada columna de tu dataset es numérica o categórica, porque:\n",
    "\n",
    "- Para numéricas → aplica Kolmogorov–Smirnov test (KS test).\n",
    "\n",
    "- Para categóricas → aplica Chi-cuadrado test u otras pruebas adecuadas.\n",
    "\n",
    "Como no le das un diccionario categories_per_feature, Alibi-Detect asume que todas las columnas son numéricas y aplica KS test en todas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541db6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las columnas son numéricas\n",
    "categories_per_feature = {i: None for i in range(X_ref.shape[1])}\n",
    "cd = TabularDrift(x_ref=X_ref, p_val=.05,categories_per_feature=categories_per_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c259a57",
   "metadata": {},
   "source": [
    "Ahora podemos comprobar si hay drift (desvio) en el conjunto de datos de prueba en comparación con el conjunto de datos de referencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc74ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cd.predict(X_test)\n",
    "labels = ['No', 'Yes']\n",
    "print('Drift: {}'.format(labels[preds['data']['is_drift']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e5655",
   "metadata": {},
   "source": [
    "`Así que no hemos detectado deriva aquí, como se esperaba.`\n",
    "\n",
    "Aunque no hubo drift en este caso, podemos simular fácilmente un escenario en el que el aparato químico utilizado para medir las propiedades químicas experimentó un error de calibración, y todos los valores se registran como un 10% más altos que sus valores reales. En este caso, si volvemos a ejecutar la detección de drift en el mismo conjunto de datos de referencia, obtendremos la siguiente salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c55b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cal_error = 1.1*X_test\n",
    "preds = cd.predict(X_test_cal_error)\n",
    "labels = ['No', 'Yes']\n",
    "print('Drift: {}'.format(labels[preds['data']['is_drift']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f9de6",
   "metadata": {},
   "source": [
    "`NOTA IMPORTANTE`: Este ejemplo es muy artificial, pero es útil para ilustrar el punto. En un conjunto de datos estándar como este, no habrá deriva de datos entre el 50% de los datos muestreados aleatoriamente y el otro 50% de los datos. Por eso, tenemos que desplazar artificialmente algunos de los puntos para mostrar que el detector realmente funciona. En escenarios del mundo real, la deriva de datos puede ocurrir de forma natural debido a todo, desde actualizaciones de sensores utilizados para mediciones; hasta cambios en el comportamiento del consumidor; pasando por cambios en el software o esquemas de bases de datos. Así que, esté atento, ya que muchos casos de deriva no serán tan fáciles de detectar como en este caso!.\n",
    "\n",
    "Este ejemplo muestra cómo, con unas pocas líneas simples de Python, podemos detectar un cambio en nuestro conjunto de datos, lo que significa que nuestro modelo de ML puede comenzar a degradarse en rendimiento si no lo reentrenamos para tener en cuenta las nuevas propiedades de los datos. También podemos utilizar técnicas similares para rastrear cuándo las métricas de rendimiento de nuestro modelo, por ejemplo, la precisión o el error cuadrático medio, también están desviándose. En este caso, debemos asegurarnos de calcular periódicamente el rendimiento en nuevos conjuntos de datos de prueba o validación. Ahora, podemos comenzar a integrar esto en soluciones que dispararán automáticamente el reentrenamiento de nuestro modelo de ML, como se muestra en el siguiente diagrama:\n",
    "\n",
    "![Un ejemplo de detección de deriva y el proceso del sistema de entrenamiento](figures/Deteccion-de-deriva.png)\n",
    "\n",
    "A continuación, veremos cómo diseñar características específicas para el consumo de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1f17d",
   "metadata": {},
   "source": [
    "# Ingenieria de caracteristicas para el consumo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88effbeb",
   "metadata": {},
   "source": [
    "Antes de alimentar cualquier dato a un modelo de aprendizaje automático, tiene que ser transformado en un estado que pueda ser entendido por nuestros modelos. También necesitamos asegurarnos de que solo hagamos esto con los datos que consideramos útiles para mejorar el rendimiento del modelo, ya que es demasiado fácil aumentar el número de características y caer en la maldición de la dimensionalidad. Esto se refiere a una serie de observaciones relacionadas donde, en problemas de alta dimensión, los datos se vuelven cada vez más dispersos en el espacio de características, por lo que lograr significancia estadística puede requerir exponencialmente más datos. En esta sección, no cubriremos la base teórica de la ingeniería de características. En cambio, nos enfocaremos en cómo nosotros, como ingenieros de ML, podemos ayudar a automatizar algunos de los pasos en producción. Para este fin, repasaremos rápidamente los principales tipos de preparación de características y pasos de ingeniería de características para que tengamos las piezas necesarias para agregar a nuestros flujos de trabajo más adelante en este capítulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240dc7c4",
   "metadata": {},
   "source": [
    "## Engineering categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7e144",
   "metadata": {},
   "source": [
    "Las características categóricas son aquellas que forman un conjunto no numérico de objetos distintos, como el día de la semana o el color de cabello. Pueden distribuirse de varias maneras a lo largo de sus datos. Para que un algoritmo de ML pueda digerir una característica categórica, necesitamos traducir la característica en algo numérico, mientras nos aseguramos de que la representación numérica no produzca sesgo o pese nuestros valores de manera inapropiada. Un ejemplo de esto sería si tuviéramos una característica que contenga diferentes productos vendidos en un supermercado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ea54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Bleach'], ['Cereal'], ['Toilet Roll']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6679f7b",
   "metadata": {},
   "source": [
    "Aquí, podemos mapear cada uno a un entero positivo utilizando el OrdinalEncoder de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "ordinal_enc = preprocessing.OrdinalEncoder()\n",
    "ordinal_enc.fit(data)\n",
    "print(ordinal_enc.transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b74167",
   "metadata": {},
   "source": [
    "Esto es lo que se llama codificación ordinal. Hemos mapeado estas características a números, así que ahí hay un gran avance, pero ¿es la representación adecuada? Bueno, si lo piensas por un segundo, no realmente. Estos números parecen sugerir que el cereal es a la lejía como el papel higiénico es al cereal, y que el promedio de papel higiénico y lejía es cereal. Estas afirmaciones no tienen sentido (y no quiero lejía y papel higiénico para el desayuno), así que esto sugiere que deberíamos intentar un enfoque diferente.\n",
    "\n",
    "Esta representación sería apropiada, sin embargo, en casos donde quisiéramos mantener la noción de orden en las características categóricas. Un excelente ejemplo sería si tuviéramos una encuesta, y se les pidiera a los participantes su opinión sobre la afirmación de que el desayuno es la comida más importante del día. Si luego se les pidiera a los participantes que seleccionaran una opción de la lista Fuertemente en desacuerdo, En desacuerdo, Ni de acuerdo ni en desacuerdo, De acuerdo y Fuertemente de acuerdo, y codificáramos ordinalmente estos datos para mapear a la lista numérica de 1, 2, 3, 4 y 5, entonces podríamos responder más intuitivamente a preguntas como si la respuesta promedio estaba más de acuerdo o en desacuerdo? y cuán extendida estaba la opinión sobre esta afirmación?. La codificación ordinal ayudaría aquí, pero como mencionamos anteriormente, no es necesariamente correcta en este caso.\n",
    "\n",
    "Lo que podríamos hacer es considerar la lista de elementos en esta característica y luego proporcionar un número binario para representar si el valor es o no ese valor particular en la lista original. Así que aquí, decidimos usar el OneHotEncoder de sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_enc = preprocessing.OneHotEncoder()\n",
    "onehot_enc.fit(data)\n",
    "print(onehot_enc.transform(data).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce79f854",
   "metadata": {},
   "source": [
    "Esta representación se conoce como codificación one-hot. Hay algunos beneficios en este método de codificación, incluidos los siguientes:\n",
    "\n",
    "- No hay ordenamientos impuestos de los valores. \n",
    "- Todos los vectores característicos tienen normas unitarias. \n",
    "- Cada característica única es ortogonal a las demás, por lo que no hay promedios o afirmaciones de distancia extrañas que estén implícitas en la representación.\n",
    "\n",
    "Una de las desventajas de este enfoque es que, si tu lista categórica contiene muchas instancias, el tamaño de tu vector de características puede aumentar rápidamente, y tenemos que almacenar y trabajar con vectores y matrices extremadamente dispersos a nivel algorítmico. Esto puede llevar fácilmente a problemas en varias implementaciones y es otra manifestación de la temida maldición de la dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29326e41",
   "metadata": {},
   "source": [
    "## Engineering numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78528593",
   "metadata": {},
   "source": [
    "Preparar características numéricas es un poco más fácil ya que ya tenemos números, pero hay algunos pasos que aún debemos seguir para prepararnos para muchos algoritmos. Para la mayoría de los algoritmos de ML, las características deben estar en escalas similares; por ejemplo, deben tener una magnitud entre -1 y 1 o entre 0 y 1. Esto es por la razón relativamente obvia de que algunos algoritmos que toman una característica para valores de precios de casas de hasta un millón de dólares y otra para la superficie en pies cuadrados de la casa automáticamente ponderarán más los valores en dólares más grandes.\n",
    "\n",
    "Esto también significa que perdemos la noción útil de dónde se ubican los valores específicos en sus distribuciones. Por ejemplo, algunos algoritmos se beneficiarán de escalar las características para que el valor medio en dólares y el valor medio en pies cuadrados estén representados ambos por 0.5 en lugar de 500,000 y 350. O puede que queramos que todas nuestras distribuciones tengan el mismo significado si estuvieran distribuidas normalmente, lo que permite que nuestros algoritmos se concentren en la forma de las distribuciones en lugar de sus ubicaciones.\n",
    "\n",
    "Entonces, ¿qué hacemos? Bueno, como siempre, no estamos empezando desde cero y hay algunas técnicas estándar que podemos aplicar. Algunas de las más comunes se enumeran aquí, pero hay muchas más para incluir todas ellas:\n",
    "\n",
    "- `Feature vector normalization`: Aquí, escalarás cada muestra en tu conjunto de datos para que tengan normas iguales a 1. Esto puede ser muy importante si estás utilizando algoritmos donde la distancia o la similitud coseno entre características es un componente importante, como en el agrupamiento. También se utiliza comúnmente en la clasificación de texto en combinación con otros métodos de ingeniería de características, como la estadística TF-IDF. En este caso, asumiendo que toda tu característica es numérica, solo necesitas calcular la norma apropiada para tu vector de características y luego dividir cada componente por ese valor.\n",
    "\n",
    "Primero, debemos importar las bibliotecas relevantes y configurar nuestros datos de entrenamiento y prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93438316",
   "metadata": {},
   "source": [
    "Entonces, debemos hacer una división típica de 70/30 entre entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e591ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =\\\n",
    "train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517fd21",
   "metadata": {},
   "source": [
    "A continuación, debemos entrenar un modelo sin ninguna estandarización en las características y predecir en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_scale_clf = make_pipeline(RidgeClassifier(tol=1e-2, solver=\"sag\"))\n",
    "no_scale_clf.fit(X_train, y_train)\n",
    "y_pred_no_scale = no_scale_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e802795",
   "metadata": {},
   "source": [
    "Finalmente, debemos hacer lo mismo pero con un paso de estandarización añadido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacfbbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale_clf = make_pipeline(StandardScaler(), RidgeClassifier(tol=1e-2,\n",
    "solver=\"sag\"))\n",
    "std_scale_clf.fit(X_train, y_train)\n",
    "y_pred_std_scale = std_scale_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e36f9",
   "metadata": {},
   "source": [
    "Ahora, si imprimimos algunas métricas de rendimiento, veremos que sin escalado, la exactitud de las predicciones es de 0.76, mientras que las otras métricas, como los promedios ponderados de precisión, recuperación y f1-score, son 0.83, 0.76 y 0.68, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAccuracy [no scaling]')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, y_pred_no_scale)))\n",
    "print('\\nClassification Report [no scaling]')\n",
    "print(metrics.classification_report(y_test, y_pred_no_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a043333e",
   "metadata": {},
   "source": [
    "En el caso en que estandarizamos los datos, las métricas son mucho mejores en general, con la precisión y los promedios ponderados de la precisión, recuperación y f1-score todos en 0.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0758708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nAccuracy [scaling]')\n",
    "print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, y_pred_std_scale)))\n",
    "print('\\nClassification Report [scaling]')\n",
    "print(metrics.classification_report(y_test, y_pred_std_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d88e149",
   "metadata": {},
   "source": [
    "Aquí, podemos ver un salto significativo en el rendimiento, simplemente añadiendo un paso simple a nuestro proceso de entrenamiento de ML. Ahora, veamos cómo funciona el entrenamiento en su esencia. Esto nos ayudará a tomar decisiones sensatas para nuestros algoritmos y enfoques de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c982ef",
   "metadata": {},
   "source": [
    "# Aprendiendo sobre el aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2b02b",
   "metadata": {},
   "source": [
    "En su esencia, los algoritmos de ML contienen una característica clave: una optimización de algún tipo. El hecho de que estos algoritmos aprendan (lo que significa que mejoran iterativamente su rendimiento en relación con una métrica apropiada al exponerse a más observaciones) es lo que los hace tan poderosos y emocionantes. Este proceso de aprendizaje es lo que nos referimos cuando decimos entrenamiento.\n",
    "\n",
    "En esta sección, cubriremos los conceptos clave que sustentan el entrenamiento, las opciones que podemos seleccionar en nuestro código y lo que esto significa para el rendimiento potencial y las capacidades de nuestro sistema de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdc0cf",
   "metadata": {},
   "source": [
    "## Definiendo el objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b916a",
   "metadata": {},
   "source": [
    "Acabamos de afirmar que el entrenamiento es una optimización, pero ¿qué estamos optimizando exactamente? Consideremos el aprendizaje supervisado. En el entrenamiento, proporcionamos las etiquetas o valores que deseamos predecir para la característica dada, de modo que los algoritmos puedan aprender la relación entre las características y el objetivo. Para optimizar los parámetros internos del algoritmo durante el entrenamiento, necesita saber cuán incorrecto estaría con su conjunto actual de parámetros. La optimización, entonces, se trata de actualizar los parámetros para que esta medida de incorrectitud sea cada vez más pequeña. Esto es exactamente lo que se capta con el concepto de una función de pérdida.\n",
    "\n",
    "Las funciones de pérdida vienen en una variedad de formas, y puedes incluso definir la tuya propia si lo necesitas con muchos paquetes, pero hay algunas estándar de las que es útil estar al tanto. Los nombres de algunas de estas se mencionan aquí.\n",
    "\n",
    "Para problemas de regresión, puedes usar lo siguiente:\n",
    "\n",
    "- Mean squared error/L2 loss\n",
    "- Mean absolute error/L1 loss\n",
    "\n",
    "Para problemas de clasificación binaria, puedes usar lo siguiente:\n",
    "\n",
    "- Log loss/logistic loss/cross-entropy loss\n",
    "- Hinge loss\n",
    "\n",
    "Para problemas de clasificación multiclase, puedes utilizar lo siguiente:\n",
    "\n",
    "- Multi-class across entropy loss\n",
    "- Kullback Leibler Divergence loss\n",
    "\n",
    "Después de definir tu función de pérdida, luego necesitas optimizarla. Esto es lo que analizaremos en la próxima sección.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e17dec",
   "metadata": {},
   "source": [
    "## Reducir tus perdidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d5ad5",
   "metadata": {},
   "source": [
    "En este punto, sabemos que el entrenamiento se trata de optimización, y sabemos qué optimizar, pero aún no hemos cubierto cómo optimizar. Como de costumbre, hay muchas opciones para elegir. En esta sección, examinaremos algunos de los enfoques principales.\n",
    "\n",
    "Las siguientes son las enfoques de tasa de aprendizaje constante:\n",
    "\n",
    "- `Gradient descent`: Este algoritmo funciona calculando la derivada de nuestra función de pérdida respecto a nuestros parámetros, y luego utiliza esto para construir una actualización que nos mueve en la dirección de disminuir la pérdida.\n",
    "\n",
    "- `Batch gradient descent`: El gradiente que usamos para hacer nuestro movimiento en el espacio de parámetros se encuentra tomando el promedio de todos los gradientes encontrados. Esto lo hace observando cada punto de datos en nuestro conjunto de entrenamiento y comprobando si el conjunto de datos no es demasiado grande y si la función de pérdida es relativamente suave y convexa. Esto puede alcanzar prácticamente el mínimo global.\n",
    "\n",
    "- `Stochastic gradient descent`: El gradiente se calcula utilizando un punto de datos seleccionado al azar en cada iteración. Esto es más rápido para llegar al mínimo global de la función de pérdida, pero es más susceptible a fluctuaciones repentinas en la pérdida después de cada paso de optimización.\n",
    "\n",
    "- `Mini-batch gradient descent`: Esta es una mezcla de los casos por lotes y estocásticos. En este caso, las actualizaciones del gradiente para cada actualización de los parámetros utilizan varios puntos mayores a uno pero menores que todo el conjunto de datos. Esto significa que el tamaño del lote es ahora un parámetro que necesita ser ajustado. Cuanto mayor es el lote, más nos acercamos al descenso de gradiente por lotes, que proporciona una mejor estimación del gradiente pero es más lento. Cuanto menor es el lote, más nos acercamos al descenso de gradiente estocástico, que es más rápido pero no tan robusto. El mini-lote nos permite decidir dónde queremos estar entre los dos. Los tamaños de lote pueden seleccionarse con una variedad de criterios en mente. Estos pueden tener en cuenta una serie de consideraciones de memoria. Los lotes procesados en paralelo y los lotes más grandes consumirán más memoria mientras proporcionan un mejor rendimiento de generalización para lotes más pequeños. Consulte el Capítulo 8 del libro Deep Learning de Ian Goodfellow, Yoshua Bengio y Aaron Courville en https://www.deeplearningbook.org/ para más detalles.\n",
    "\n",
    "Luego, están los métodos de tasa de aprendizaje adaptativa. Algunos de los más comunes son los siguientes:\n",
    "\n",
    "- `AdaGrad`: Los parámetros de la tasa de aprendizaje se actualizan dinámicamente en función de las propiedades de las actualizaciones de aprendizaje durante el proceso de optimización.\n",
    "- `AdaDelta`: Esta es una extensión de AdaGrad que no utiliza todas las actualizaciones de gradiente anteriores. En cambio, utiliza una ventana deslizante sobre las actualizaciones.\n",
    "- `RMSprop`: Esto funciona manteniendo un promedio móvil del cuadrado de todos los pasos del gradiente. Luego, divide el último gradiente por la raíz cuadrada de esto.\n",
    "- `Adam`: Este es un algoritmo que se supone que combina los beneficios de AdaGrad y RMSprop.\n",
    "\n",
    "Los límites y capacidades de todos estos enfoques de optimización son importantes para nosotros, como ingenieros de aprendizaje automático, porque queremos asegurar que nuestros sistemas de entrenamiento utilicen la herramienta adecuada para el trabajo y sean óptimos para el problema en cuestión. Simplemente tener la conciencia de que hay múltiples opciones para su optimización interna también te ayudará a enfocar tus esfuerzos y aumentar el rendimiento.\n",
    "\n",
    "Ahora, pensemos en qué nivel de control podemos tener sobre el proceso de entrenamiento mientras desarrollamos nuestras soluciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660ef20",
   "metadata": {},
   "source": [
    "## Jerarquias de automatizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9de0dd",
   "metadata": {},
   "source": [
    "Una de las principales razones por las que el aprendizaje automático (ML) es ahora una parte común del desarrollo de software, así como una actividad empresarial y académica importante, es debido a la plétora de herramientas disponibles en la actualidad. Todos los paquetes y bibliotecas que contienen implementaciones funcionales y optimizadas de algoritmos sofisticados han permitido a las personas construir sobre ellos, en lugar de tener que reimplementar lo básico cada vez que hay un problema por resolver. Esta es una poderosa expresión de la idea de abstracción en el desarrollo de software, donde se pueden aprovechar y utilizar unidades de nivel inferior en niveles más altos de implementación.\n",
    "\n",
    "Esta idea se puede ampliar aún más a toda la empresa de la capacitación en sí. En el nivel más bajo de implementación (pero aún muy alto en el sentido de los algoritmos subyacentes), podemos proporcionar detalles sobre cómo queremos que avance el proceso de capacitación. Podemos definir manualmente el conjunto exacto de hiperparámetros (ver la siguiente sección sobre Optimización de hiperparámetros) que usar en la ejecución de la capacitación en nuestro código. Yo llamo a esto 'manipular a mano'. Luego podemos pasar un nivel de abstracción superior y proporcionar rangos y límites para nuestros hiperparámetros a herramientas diseñadas para muestrear y probar de manera eficiente el rendimiento de nuestro modelo para cada uno de estos; por ejemplo, la sintonización automática de hiperparámetros. Finalmente, hay un nivel de abstracción aún más alto que ha creado mucha emoción mediática en los últimos años, donde optimizamos sobre qué algoritmo ejecutar. Esto es conocido como ML automatizado o AutoML.\n",
    "\n",
    "Hay mucho bombo alrededor de AutoML, con algunas personas proclamando la eventual automatización de todos los roles laborales en el desarrollo de ML. En mi opinión, esto simplemente no es realista, ya que seleccionar tu modelo y los hiperparámetros es solo un aspecto de un desafío de ingeniería enormemente complejo (de ahí que esto sea un libro y no un folleto). Sin embargo, AutoML es una herramienta muy poderosa que debería añadirse a tu arsenal de capacidades cuando te enfrentes a tu próximo proyecto de ML.\n",
    "\n",
    "Podemos resumir todo esto de manera bastante sencilla como una jerarquía de automatización; básicamente, ¿cuánto control quieres tú, como ingeniero de ML, en el proceso de entrenamiento? Una vez escuché que esto se describía en términos de control de marchas en un coche (crédito: Databricks en Spark AI 2019). Manejar a mano es el equivalente de conducir un coche manual, con control total sobre las marchas: hay más en qué pensar, pero puede ser muy eficiente si sabes lo que estás haciendo. Un nivel más arriba, tienes coches automáticos: hay menos de qué preocuparse, para que puedas concentrarte más en llegar a tu destino, el tráfico y otros desafíos.\n",
    "\n",
    "Esta es una buena opción para muchas personas, pero aún requiere que tengas suficientes conocimientos, habilidades y comprensión. Finalmente, tenemos coches autónomos: siéntate, relájate y ni siquiera te preocupes por cómo llegar a donde vas. Puedes centrarte en lo que vas a hacer una vez que llegues allí."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22331c",
   "metadata": {},
   "source": [
    "# Optimizing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a05a6",
   "metadata": {},
   "source": [
    "Una de las principales razones por las que el aprendizaje automático (ML) es ahora una parte común del desarrollo de software, así como una actividad empresarial y académica importante, es debido a la plétora de herramientas disponibles en la actualidad. Todos los paquetes y bibliotecas que contienen implementaciones funcionales y optimizadas de algoritmos sofisticados han permitido a las personas construir sobre ellos, en lugar de tener que reimplementar lo básico cada vez que hay un problema por resolver. Esta es una poderosa expresión de la idea de abstracción en el desarrollo de software, donde se pueden aprovechar y utilizar unidades de nivel inferior en niveles más altos de implementación.\n",
    "\n",
    "Cuando ajustas algún tipo de función matemática a los datos, algunos valores se ajustan durante el procedimiento de ajuste o entrenamiento: estos se llaman parámetros. Para el aprendizaje automático, hay un nivel adicional de abstracción donde tenemos que definir los valores que informan a los algoritmos que estamos empleando cómo deben actualizar los parámetros. Estos valores se llaman hiperparámetros, y su selección es una de las importantes artes ocultas del entrenamiento de algoritmos de aprendizaje automático.\n",
    "\n",
    "Las siguientes tablas enumeran algunos hiperparámetros que se utilizan para algoritmos comunes de ML para mostrarte las diferentes formas que pueden tomar. Estas listas no son exhaustivas, pero están ahí para resaltar que la optimización de hiperparámetros no es un ejercicio trivial:\n",
    "\n",
    "| Algorithm                   | Hyperparameters                               | What This Controls                          |\n",
    "|------------------------------|-----------------------------------------------|---------------------------------------------|\n",
    "| Decision Trees and Random Forests | Tree depth<br> Min/max leaves | Number of levels<br> Amount of branching at each level |\n",
    "| Support Vector Machines (SVM) | C<br> Gamma | Penalty for misclassification<br> Radius of influence (RBF kernel) |\n",
    "| Neural Networks              | Learning rate<br> Number of hidden layers<br> Activation function | Update step size<br> Depth of the network<br> Neuron activation conditions |\n",
    "| Logistic Regression          | Solver<br> Regularization type<br> Regularization strength | How to minimize the loss<br> How to prevent overfitting<br> Strength of regularization |\n",
    "\n",
    "\n",
    "Todos estos hiperparámetros tienen su propio conjunto específico de valores que pueden tomar. Este rango de valores de hiperparámetros para los diferentes algoritmos potenciales que deseas aplicar a tu solución de ML significa que hay muchas maneras de definir un modelo funcional (es decir, uno que no rompa la implementación que estás utilizando), pero ¿cómo encuentras el modelo óptimo?\n",
    "\n",
    "Aquí es donde entra la búsqueda de hiperparámetros. El concepto es que, para un número finito de combinaciones de valores de hiperparámetros, queremos encontrar el conjunto que proporcione el mejor rendimiento del modelo. ¡Este es otro problema de optimización que es similar al de la capacitación en primer lugar!\n",
    "\n",
    "En las siguientes secciones, discutiremos dos bibliotecas de optimización de hiperparámetros muy populares y te mostraremos cómo implementarlas en unas pocas líneas de Python.\n",
    "\n",
    "`NOTA IMPORTANTE`: Es importante entender qué algoritmos se están utilizando para la optimización en estas bibliotecas de hiperparámetros, ya que puedes querer usar un par de implementaciones diferentes de cada una para comparar diferentes enfoques y evaluar el rendimiento. Si no observaste cómo funcionan internamente, podrías hacer comparaciones injustas con facilidad, o peor, podrías estar comparando casi lo mismo sin saberlo. Si tienes un conocimiento más profundo de cómo funcionan estas soluciones, también podrás tomar mejores decisiones sobre cuándo serán beneficiosas y cuándo serán excesivas. Aspira a tener un conocimiento funcional de algunos de estos algoritmos y enfoques, ya que esto te ayudará a diseñar sistemas de entrenamiento más holísticos con enfoques de ajuste de algoritmos que se complementen entre sí.\n",
    "\n",
    "- `Hyperopt`: Hyperopt es un paquete de Python de código abierto que se presenta como adecuado para la optimización en serie y paralela sobre espacios de búsqueda complicados, que pueden incluir dimensiones de valor real, discretas y condicionales. Consulta el siguiente enlace para más información: https://github.com/Hyperopt/Hyperopt.  En el momento de escribir, la versión 0.2.5 viene empaquetada con tres algoritmos para realizar optimización sobre espacios de búsqueda proporcionados por el usuario:\n",
    "    - Random search: Este algoritmo, esencialmente, selecciona números aleatorios dentro de los rangos de valores de parámetros que has proporcionado y los prueba. Luego evalúa qué conjuntos de números ofrecen el mejor rendimiento de acuerdo con la función objetivo que has elegido.\n",
    "    - Tree of Parzen Estimators (TPE): Este es un enfoque de optimización bayesiana que modela distribuciones de hiperparámetros por debajo y por encima de un umbral para la función objetivo (aproximadamente buenos y malos puntajes), y luego busca extraer más valores de la distribución de hiperparámetros buenos.\n",
    "    - Adaptive TPE: Esta es una versión modificada de TPE que permite cierta optimización de la búsqueda, así como la posibilidad de crear un modelo de ML para ayudar a guiar el proceso de optimización.\n",
    "\n",
    "El repositorio y la documentación de Hyperopt contienen varios ejemplos trabajados agradables y detallados. No vamos a repasarlos aquí. En su lugar, aprenderemos cómo usar esto para un modelo de clasificación sencillo, como el que definimos en el Capítulo 1, Introducción a la Ingeniería de ML. Comencemos:\n",
    "\n",
    "1. En Hyperopt, debemos definir los hiperparámetros que queremos optimizar. Por ejemplo, para un problema típico de regresión logística, podríamos definir el espacio de hiperparámetros a cubrir, si deseamos reutilizar los parámetros que se aprendieron de las ejecuciones anteriores del modelo cada vez (warm_start), si queremos que el modelo incluya un sesgo en la función de decisión (fit_intercept), la tolerancia establecida para decidir cuándo detener la optimización (tol), el parámetro de regularización (C), qué solver queremos probar y el número máximo de iteraciones, max_iter, en cualquier ejecución de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aea1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "space = {\n",
    "'warm_start' : hp.choice('warm_start', [True, False]),\n",
    "'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
    "'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
    "'C' : hp.uniform('C', 0.05, 2.5),\n",
    "'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear']),\n",
    "'max_iter' : hp.choice('max_iter', range(10,500))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443afcb7",
   "metadata": {},
   "source": [
    "Luego, tenemos que definir una función objetivo para optimizar. En el caso de nuestro algoritmo de clasificación, podemos simplemente definir la función de pérdida que queremos minimizar como el f1-score. Ten en cuenta que Hyperopt permite que tu función objetivo proporcione estadísticas de ejecución y metadatos a través de tu declaración de retorno si estás utilizando la funcionalidad fmin. El único requisito si haces esto es que devuelvas un valor etiquetado como pérdida y un valor de estado válido de la lista de Hyperopt.STATUS_STRING (ok por defecto y fail si hay un problema en el cálculo que deseas señalar como un fallo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from functools import partial\n",
    "\n",
    "def objective(params, n_folds, X, y):\n",
    "    clf = LogisticRegression(**params, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=n_folds, scoring='f1_macro')\n",
    "    max_score = max(scores)\n",
    "    loss = 1 - max_score\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b4866",
   "metadata": {},
   "source": [
    "Ahora, debemos optimizar usando el método fmin con el algoritmo TPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694491be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trials = Trials()\n",
    "n_folds = 5\n",
    "# Optimize\n",
    "best = fmin(fn=partial(objective, n_folds=n_folds, \n",
    "                       X=X_train, y=y_train),space=space,\n",
    "                       algo=tpe.suggest,max_evals=16,trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c29e0d",
   "metadata": {},
   "source": [
    "El mejor es un diccionario que contiene todos los mejores hiperparámetros en el espacio de búsqueda que definiste. Así que, en este caso, tenemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ea333",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'C': 0.26895003542493234,\n",
    "'fit_intercept': 1,\n",
    "'max_iter': 452,\n",
    "'solver': 2,\n",
    "'tol': 1.863336145787027e-05,\n",
    "'warm_start': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcaa14",
   "metadata": {},
   "source": [
    "Luego puedes usar estos hiperparámetros para definir tu modelo para entrenarlo con los datos.\n",
    "\n",
    "- `Optuna`: Optuna es un paquete de software que tiene una amplia serie de capacidades basadas en algunos principios de diseño fundamentales, como su API definida por ejecución y su arquitectura modular. Definido por ejecución se refiere al hecho de que, al usar Optuna, el usuario no tiene que definir el conjunto completo de parámetros a probar, lo que sería definido y ejecutado. En su lugar, pueden proporcionar algunos valores iniciales y pedirle a Optuna que sugiera su propio conjunto de experimentos para ejecutar. Esto ahorra tiempo al usuario y reduce la huella de código (¡dos grandes ventajas para mí!).\n",
    "\n",
    "Optuna contiene cuatro algoritmos de búsqueda básicos: búsqueda en cuadrícula, búsqueda aleatoria, TPE y el algoritmo de Estrategia de Evolución de Adaptación de Matriz de Covarianza (CMA-ES). Ya cubrimos los tres primeros anteriormente, pero CMA-ES es una adición importante al conjunto. Como su nombre sugiere, se basa en un algoritmo evolutivo y toma muestras de hiperparámetros de una distribución gaussiana multivariada. Luego, utiliza las clasificaciones de los puntajes evaluados para la función objetivo dada para actualizar dinámicamente los parámetros de la distribución gaussiana (la matriz de covarianza siendo un conjunto de estos) para ayudar a encontrar un óptimo en el espacio de búsqueda de manera rápida y robusta.\n",
    "\n",
    "Sin embargo, la clave que hace que el proceso de optimización de Optuna sea diferente al de Hyperopt es su aplicación de la poda o detención temprana automatizada. Durante la optimización, si Optuna detecta evidencia de que un ensayo de un conjunto de hiperparámetros no conducirá a un algoritmo entrenado en general mejor, termina ese ensayo. Los desarrolladores del paquete sugieren que esto lleva a ganancias de eficiencia en el proceso de optimización de hiperparámetros al reducir la computación innecesaria.\n",
    "\n",
    "Aquí, estamos viendo el mismo ejemplo que vimos anteriormente, pero ahora estamos usando Optuna en lugar de Hyperopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8bfcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial, n_folds, X, y):\n",
    "    params = {\n",
    "        'warm_start': trial.suggest_categorical('warm_start', [True, False]),\n",
    "        'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'tol': trial.suggest_float('tol', 1e-5, 1e-4),   # antes suggest_uniform\n",
    "        'C': trial.suggest_float('C', 0.05, 2.5),       # antes suggest_uniform\n",
    "        'solver': trial.suggest_categorical('solver', ['newton-cg', 'lbfgs', 'liblinear']),\n",
    "        'max_iter': trial.suggest_int('max_iter', 10, 500)  # mejor que categorical(range)\n",
    "    }\n",
    "\n",
    "    # Modelo con hiperparámetros sugeridos\n",
    "    clf = LogisticRegression(**params, random_state=42)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_val_score(clf, X, y, cv=n_folds, scoring='f1_macro')\n",
    "    \n",
    "    # Pérdida a minimizar\n",
    "    loss = 1 - max(scores)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d658c13",
   "metadata": {},
   "source": [
    "Ahora, debemos configurar los datos de la misma manera que lo hicimos en el ejemplo de Hyperopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "n_folds = 5\n",
    "X, y = datasets.make_classification(n_samples=100000, n_features=20,n_informative=2,\n",
    "n_redundant=2)\n",
    "train_samples = 100\n",
    "# Samples used for training the models\n",
    "X_train = X[:train_samples]\n",
    "X_test = X[train_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_test = y[train_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484f38e",
   "metadata": {},
   "source": [
    "Ahora, podemos definir este objeto de Estudio que mencionamos y decirle cómo deseamos optimizar el valor que devuelve nuestra función objetivo, con instrucciones sobre cuántas pruebas realizar en el estudio. Aquí, utilizaremos nuevamente el algoritmo de muestreo TPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975cc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import TPESampler\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
    "study.optimize(partial(objective, n_folds=n_folds, X=X_train, y=y_train), n_trials=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75618c7b",
   "metadata": {},
   "source": [
    "Ahora, podemos acceder a los mejores parámetros a través de la variable study.best_trial.params, que nos da los siguientes valores para el mejor caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085884a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d8e87",
   "metadata": {},
   "source": [
    "`NOTA IMPORTANTE`: Notarás que estos valores son diferentes de los devueltos por Hyperopt. Esto se debe a que solo hemos realizado 16 ensayos en cada caso, por lo que no estamos muestreando efectivamente el espacio. Si ejecutas cualquiera de las muestras de Hyperopt o Optuna varias veces seguidas, puedes obtener resultados bastante diferentes por la misma razón. El ejemplo dado aquí es solo para mostrar la sintaxis, pero si estás interesado, puedes establecer el número de iteraciones en un valor muy alto (o crear espacios más pequeños para muestrear), y los resultados de los dos enfoques deberían converger.\n",
    "\n",
    "- `AutoML`: El nivel final de nuestra jerarquía es aquel donde nosotros, como ingenieros, tenemos el menor control directo sobre el proceso de entrenamiento, ¡pero donde también podríamos obtener una buena respuesta con muy poco esfuerzo! El tiempo de desarrollo que se requiere para buscar entre muchos hiperparámetros y algoritmos para tu problema puede ser grande, incluso cuando programas parámetros de búsqueda y bucles que parecen razonables.\n",
    "\n",
    "Dado esto, en los últimos años se han despliegueado varias bibliotecas y herramientas de AutoML en una variedad de lenguajes y ecosistemas de software. La exageración en torno a estas técnicas ha significado que han tenido una gran cantidad de atención, lo que ha llevado a varios científicos de datos a cuestionar cuándo sus trabajos serán automatizados. Como mencionamos anteriormente en este capítulo, en mi opinión, declarar la muerte de la ciencia de datos es extremadamente prematuro y también peligroso desde el punto de vista organizacional y del rendimiento empresarial. Estas herramientas han sido otorgadas un estatus pseudo-mitológico tal que muchas empresas podrían creer que simplemente usarlas unas pocas veces resolverá todos sus problemas de ciencia de datos y ML.\n",
    "\n",
    "Están equivocados, pero también tienen razón.\n",
    "\n",
    "Estas herramientas y técnicas son muy poderosas y pueden ayudar a mejorar algunas cosas, pero no son una panacea mágica que se pueda usar de manera inmediata. Exploremos estas herramientas y comencemos a pensar en cómo incorporarlas en nuestro flujo de trabajo y soluciones de ingeniería de ML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a49962",
   "metadata": {},
   "source": [
    "# Auto-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6151a438",
   "metadata": {},
   "source": [
    "Una de nuestras bibliotecas favoritas, el buen viejo scikit-learn, siempre iba a ser uno de los primeros objetivos para construir una biblioteca de AutoML popular. Una de las características muy poderosas de auto-sklearn es que su API ha sido diseñada para que los objetos principales que optimizan y seleccionan modelos y hiperparámetros puedan ser intercambiados sin problemas en tu código.\n",
    "\n",
    "Como de costumbre, un ejemplo lo mostrará con mayor claridad. En el siguiente ejemplo, asumiremos que el conjunto de datos de vino (un favorito para este capítulo) ya ha sido recuperado y dividido en muestras de entrenamiento y prueba, de acuerdo con otros ejemplos, como el que se presenta en la sección de Detección de deriva:\n",
    "\n",
    "1. Primero, dado que este es un problema de clasificación, lo principal que necesitamos obtener de auto-sklearn es el objeto autosklearn.classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4a8f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:07] {1752} INFO - task = classification\n",
      "[flaml.automl.logger: 09-09 18:39:07] {1763} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 09-09 18:39:07] {1862} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.logger: 09-09 18:39:07] {1979} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 09-09 18:39:07] {2282} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:07] {2417} INFO - Estimated sufficient time budget=672s. Estimated necessary time budget=17s.\n",
      "[flaml.automl.logger: 09-09 18:39:07] {2466} INFO -  at 0.1s,\testimator lgbm's best error=0.1051,\tbest estimator lgbm's best error=0.1051\n",
      "[flaml.automl.logger: 09-09 18:39:07] {2282} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:07] {2466} INFO -  at 0.1s,\testimator lgbm's best error=0.1051,\tbest estimator lgbm's best error=0.1051\n",
      "[flaml.automl.logger: 09-09 18:39:07] {2282} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.2s,\testimator lgbm's best error=0.0524,\tbest estimator lgbm's best error=0.0524\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 3, current learner sgd\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.3s,\testimator sgd's best error=0.6017,\tbest estimator lgbm's best error=0.0524\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 4, current learner lgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.3s,\testimator lgbm's best error=0.0299,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.4s,\testimator lgbm's best error=0.0299,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.4s,\testimator lgbm's best error=0.0299,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.4s,\testimator lgbm's best error=0.0299,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 8, current learner xgboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.7s,\testimator xgboost's best error=0.0900,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.8s,\testimator lgbm's best error=0.0299,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 10, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 0.9s,\testimator extra_tree's best error=0.0897,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 11, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:08] {2466} INFO -  at 1.1s,\testimator rf's best error=0.0969,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:08] {2282} INFO - iteration 12, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 1.2s,\testimator rf's best error=0.0823,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 1.3s,\testimator xgboost's best error=0.0598,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 14, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 1.5s,\testimator extra_tree's best error=0.0897,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 1.5s,\testimator xgboost's best error=0.0598,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 16, current learner sgd\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 1.7s,\testimator sgd's best error=0.3538,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 1.7s,\testimator lgbm's best error=0.0299,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 18, current learner sgd\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 1.8s,\testimator sgd's best error=0.3538,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 1.8s,\testimator lgbm's best error=0.0299,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 20, current learner extra_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 2.0s,\testimator extra_tree's best error=0.0897,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 2.0s,\testimator xgboost's best error=0.0598,\tbest estimator lgbm's best error=0.0299\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 22, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 2.1s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2466} INFO -  at 2.1s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:09] {2282} INFO - iteration 24, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.2s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 25, current learner extra_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.3s,\testimator extra_tree's best error=0.0897,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 26, current learner sgd\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.4s,\testimator sgd's best error=0.3538,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 27, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.5s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.5s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.7s,\testimator extra_tree's best error=0.0672,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 30, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.8s,\testimator extra_tree's best error=0.0670,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.9s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 2.9s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 33, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 3.0s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 34, current learner xgboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:10] {2466} INFO -  at 3.1s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:10] {2282} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.2s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 36, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.3s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 37, current learner sgd\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.3s,\testimator sgd's best error=0.3538,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 38, current learner xgboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.4s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.5s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 40, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.6s,\testimator rf's best error=0.0746,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 41, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.7s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 42, current learner sgd\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.7s,\testimator sgd's best error=0.3538,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 43, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.8s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 44, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 3.8s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 45, current learner sgd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 4.0s,\testimator sgd's best error=0.3536,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 46, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 4.0s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 47, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2466} INFO -  at 4.1s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:11] {2282} INFO - iteration 48, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 4.1s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 4.2s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 50, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 4.4s,\testimator rf's best error=0.0450,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 51, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 4.4s,\testimator xgboost's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 52, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 4.6s,\testimator rf's best error=0.0450,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 53, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 4.8s,\testimator rf's best error=0.0450,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 4.8s,\testimator lgbm's best error=0.0299,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 4.8s,\testimator lgbm's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 56, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 5.0s,\testimator rf's best error=0.0450,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 57, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 5.1s,\testimator lgbm's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 58, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2466} INFO -  at 5.1s,\testimator lgbm's best error=0.0296,\tbest estimator xgboost's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:12] {2282} INFO - iteration 59, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 5.3s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 60, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 5.3s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 61, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 5.5s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 62, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 5.5s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 63, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 5.7s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 64, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 5.8s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 65, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 5.8s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 66, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 6.0s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 67, current learner xgboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:13] {2466} INFO -  at 6.1s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:13] {2282} INFO - iteration 68, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2466} INFO -  at 6.2s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2282} INFO - iteration 69, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2466} INFO -  at 6.3s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2282} INFO - iteration 70, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:14] {2466} INFO -  at 6.5s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2282} INFO - iteration 71, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2466} INFO -  at 6.6s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2282} INFO - iteration 72, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2466} INFO -  at 6.7s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2282} INFO - iteration 73, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2466} INFO -  at 6.8s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2282} INFO - iteration 74, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2466} INFO -  at 7.0s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2282} INFO - iteration 75, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2466} INFO -  at 7.1s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:14] {2282} INFO - iteration 76, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.2s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 77, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.2s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 78, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.3s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 79, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.4s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 80, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.5s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 81, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.6s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 82, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.6s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 83, current learner xgboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.7s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 84, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.8s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 85, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 7.8s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 86, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 8.0s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 87, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 8.1s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2282} INFO - iteration 88, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:15] {2466} INFO -  at 8.1s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 89, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 8.2s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 90, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 8.3s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 91, current learner lgbm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 8.3s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 92, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 8.5s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 93, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 8.6s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 94, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 8.8s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 95, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 8.9s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 96, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 8.9s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 97, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2466} INFO -  at 9.0s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:16] {2282} INFO - iteration 98, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:17] {2466} INFO -  at 9.2s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2282} INFO - iteration 99, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2466} INFO -  at 9.3s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2282} INFO - iteration 100, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2466} INFO -  at 9.3s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2282} INFO - iteration 101, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2466} INFO -  at 9.4s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2282} INFO - iteration 102, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:17] {2466} INFO -  at 9.6s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2282} INFO - iteration 103, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2466} INFO -  at 9.6s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:17] {2282} INFO - iteration 104, current learner catboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:55] {2466} INFO -  at 47.9s,\testimator catboost's best error=0.0373,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:55] {2282} INFO - iteration 105, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:55] {2466} INFO -  at 48.0s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:55] {2282} INFO - iteration 106, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.2s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 107, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.3s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 108, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.4s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 109, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.4s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 110, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.5s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 111, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.7s,\testimator rf's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 112, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.8s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 113, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.8s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 114, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 48.9s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 115, current learner extra_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 49.0s,\testimator extra_tree's best error=0.0670,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 116, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2466} INFO -  at 49.1s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:56] {2282} INFO - iteration 117, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.2s,\testimator lgbm's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 118, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.3s,\testimator xgboost's best error=0.0296,\tbest estimator rf's best error=0.0296\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 119, current learner xgboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.3s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 120, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.5s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 121, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.5s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 122, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.6s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 123, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 124, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 125, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 49.9s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 126, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2466} INFO -  at 50.0s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:57] {2282} INFO - iteration 127, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 50.2s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 128, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 50.4s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 129, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 50.5s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 130, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 50.6s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 131, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 50.6s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 132, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 50.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 133, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 50.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 134, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 50.9s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 135, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 51.1s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 136, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2466} INFO -  at 51.1s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:58] {2282} INFO - iteration 137, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.1s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 138, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.2s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 139, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.3s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 140, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.4s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 141, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.6s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 142, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.6s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 143, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.6s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 144, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 145, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 146, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 147, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 51.9s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 148, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2466} INFO -  at 52.1s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:39:59] {2282} INFO - iteration 149, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2466} INFO -  at 52.3s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2282} INFO - iteration 150, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2466} INFO -  at 52.4s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2282} INFO - iteration 151, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2466} INFO -  at 52.6s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2282} INFO - iteration 152, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2466} INFO -  at 52.8s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2282} INFO - iteration 153, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2466} INFO -  at 52.8s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2282} INFO - iteration 154, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2466} INFO -  at 52.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2282} INFO - iteration 155, current learner lgbm\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2466} INFO -  at 52.9s,\testimator lgbm's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2282} INFO - iteration 156, current learner rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-09 18:40:00] {2466} INFO -  at 53.1s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:00] {2282} INFO - iteration 157, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.2s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 158, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.2s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 159, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.4s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 160, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.6s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 161, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.6s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 162, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.8s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 163, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.8s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 164, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 165, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 53.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 166, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2466} INFO -  at 54.1s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:01] {2282} INFO - iteration 167, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.1s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 168, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.2s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 169, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.3s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 170, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.4s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 171, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.6s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 172, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.7s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 173, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.8s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 174, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.8s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 175, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 176, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 54.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 177, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 55.1s,\testimator extra_tree's best error=0.0598,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 178, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2466} INFO -  at 55.1s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:02] {2282} INFO - iteration 179, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.2s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 180, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.2s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 181, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.4s,\testimator extra_tree's best error=0.0598,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 182, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.4s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 183, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.5s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 184, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.6s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 185, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 186, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.7s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 187, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.8s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 188, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 55.8s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 189, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 56.0s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 190, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2466} INFO -  at 56.1s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:03] {2282} INFO - iteration 191, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2466} INFO -  at 56.3s,\testimator extra_tree's best error=0.0524,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2282} INFO - iteration 192, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2466} INFO -  at 56.3s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2282} INFO - iteration 193, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2466} INFO -  at 56.5s,\testimator extra_tree's best error=0.0373,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2282} INFO - iteration 194, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2466} INFO -  at 56.6s,\testimator extra_tree's best error=0.0373,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2282} INFO - iteration 195, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2466} INFO -  at 56.8s,\testimator extra_tree's best error=0.0373,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2282} INFO - iteration 196, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2466} INFO -  at 56.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2282} INFO - iteration 197, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2466} INFO -  at 57.0s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:04] {2282} INFO - iteration 198, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2466} INFO -  at 57.2s,\testimator extra_tree's best error=0.0373,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2282} INFO - iteration 199, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2466} INFO -  at 57.4s,\testimator extra_tree's best error=0.0373,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2282} INFO - iteration 200, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2466} INFO -  at 57.5s,\testimator extra_tree's best error=0.0373,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2282} INFO - iteration 201, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2466} INFO -  at 57.7s,\testimator extra_tree's best error=0.0373,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2282} INFO - iteration 202, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2466} INFO -  at 57.9s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2282} INFO - iteration 203, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2466} INFO -  at 58.0s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:05] {2282} INFO - iteration 204, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.2s,\testimator extra_tree's best error=0.0373,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 205, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.4s,\testimator extra_tree's best error=0.0299,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 206, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.6s,\testimator extra_tree's best error=0.0299,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 207, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.6s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 208, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.8s,\testimator extra_tree's best error=0.0299,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 209, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.8s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 210, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 211, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 212, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 58.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 213, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2466} INFO -  at 59.0s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:06] {2282} INFO - iteration 214, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 59.2s,\testimator extra_tree's best error=0.0299,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 215, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 59.3s,\testimator extra_tree's best error=0.0299,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 216, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 59.4s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 217, current learner extra_tree\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 59.5s,\testimator extra_tree's best error=0.0299,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 218, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 59.7s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 219, current learner rf\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 59.8s,\testimator rf's best error=0.0296,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 220, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 59.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 221, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 59.9s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 222, current learner xgboost\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 60.0s,\testimator xgboost's best error=0.0222,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 223, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 60.0s,\testimator xgb_limitdepth's best error=0.1048,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2282} INFO - iteration 224, current learner lrl1\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2466} INFO -  at 60.1s,\testimator lrl1's best error=0.3459,\tbest estimator xgboost's best error=0.0222\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2724} INFO - retrain xgboost for 0.0s\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2727} INFO - retrained model: XGBClassifier(base_score=None, booster=None, callbacks=[],\n",
      "              colsample_bylevel=0.6316592828288627, colsample_bynode=None,\n",
      "              colsample_bytree=1.0, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              feature_weights=None, gamma=None, grow_policy='lossguide',\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.004518795699787654, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=0, max_leaves=5,\n",
      "              min_child_weight=0.36830299605214667, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=4,\n",
      "              n_jobs=-1, num_parallel_tree=None, ...)\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2009} INFO - fit succeeded\n",
      "[flaml.automl.logger: 09-09 18:40:07] {2010} INFO - Time taken to find the best model: 49.32375502586365\n",
      "Mejor modelo: <flaml.automl.model.XGBoostSklearnEstimator object at 0x72108da824e0>\n",
      "Mejor config: {'n_estimators': 4, 'max_leaves': 5, 'min_child_weight': 0.36830299605214667, 'learning_rate': 0.004518795699787654, 'subsample': 1.0, 'colsample_bylevel': 0.6316592828288627, 'colsample_bytree': 1.0, 'reg_alpha': 0.002839187489003967, 'reg_lambda': 4.408246499749567}\n",
      "Mejor score (validación): 0.022222222222222233\n",
      "Accuracy en test: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from flaml import AutoML\n",
    "\n",
    "# Datos\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Configuración de AutoML\n",
    "automl = AutoML()\n",
    "\n",
    "settings = {\n",
    "    \"time_budget\": 60,  # tiempo total en segundos\n",
    "    \"task\": \"classification\",\n",
    "    \"metric\": \"accuracy\",\n",
    "    \"log_file_name\": \"wine.log\",\n",
    "}\n",
    "\n",
    "# Entrenamiento\n",
    "automl.fit(X_train, y_train, **settings)\n",
    "\n",
    "# Modelos y estadísticas\n",
    "print(\"Mejor modelo:\", automl.model)\n",
    "print(\"Mejor config:\", automl.best_config)\n",
    "print(\"Mejor score (validación):\", automl.best_loss)\n",
    "\n",
    "# Predicción\n",
    "predictions = automl.predict(X_test)\n",
    "print(\"Accuracy en test:\", accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1d800",
   "metadata": {},
   "source": [
    "Como puedes ver, es muy sencillo comenzar a usar esta poderosa biblioteca, especialmente si ya te sientes cómodo trabajando con sklearn.\n",
    "\n",
    "A continuación, discutamos cómo extendemos este concepto a las redes neuronales, que tienen una capa extra de complejidad debido a sus diferentes arquitecturas de modelo potenciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129b4fe",
   "metadata": {},
   "source": [
    "## Optuna en redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c3a72",
   "metadata": {},
   "source": [
    "Un área particular donde AutoML ha tenido un gran impacto es en las redes neuronales. Esto se debe a que, para una red neuronal, la pregunta de cuál es el mejor modelo es una cuestión muy complicada. Para nuestros clasificadores típicos, generalmente podemos pensar en una lista relativamente corta y finita de algoritmos a probar. Para una red neuronal, no tenemos esta lista finita. En cambio, tenemos un conjunto esencialmente infinito de posibles arquitecturas de redes neuronales; por ejemplo, para organizar las neuronas en capas y las conexiones entre ellas. Buscar la arquitectura óptima de una red neuronal es un problema en el que una optimización poderosa puede facilitarte la vida, como ingeniero de ML o científico de datos.\n",
    "\n",
    "En este caso, vamos a explorar a optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72e4505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-10 10:08:43,959] A new study created in memory with name: no-name-5c5551c5-a37a-4ab2-b948-b2a25ea60561\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1757516924.526745   25203 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6042 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2025-09-10 10:08:46.107169: I external/local_xla/xla/service/service.cc:163] XLA service 0x7f036c00b150 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-09-10 10:08:46.107194: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 with Max-Q Design, Compute Capability 7.5\n",
      "2025-09-10 10:08:46.138026: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-09-10 10:08:46.311596: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91300\n",
      "I0000 00:00:1757516927.502787   26593 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2025-09-10 10:08:51,126] Trial 0 finished with value: 1.0 and parameters: {'n_layers': 3, 'units': 16, 'dropout': 0.07802871566412534, 'lr': 0.007825365905628434}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:08:56,392] Trial 1 finished with value: 1.0 and parameters: {'n_layers': 2, 'units': 80, 'dropout': 0.20546466880926534, 'lr': 0.0008005731740727337}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:02,148] Trial 2 finished with value: 1.0 and parameters: {'n_layers': 3, 'units': 96, 'dropout': 0.12366006750253045, 'lr': 0.00047657943044019524}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:06,854] Trial 3 finished with value: 1.0 and parameters: {'n_layers': 2, 'units': 80, 'dropout': 0.4392190989681465, 'lr': 0.0012492597643957819}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:12,881] Trial 4 finished with value: 1.0 and parameters: {'n_layers': 3, 'units': 112, 'dropout': 0.3061606627792066, 'lr': 0.0033776010188822096}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:17,175] Trial 5 finished with value: 1.0 and parameters: {'n_layers': 1, 'units': 64, 'dropout': 0.2190991372758342, 'lr': 0.0010902151161416263}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:21,061] Trial 6 finished with value: 0.9444444179534912 and parameters: {'n_layers': 1, 'units': 96, 'dropout': 0.28599120279271534, 'lr': 0.00022926238060433197}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:26,635] Trial 7 finished with value: 1.0 and parameters: {'n_layers': 3, 'units': 48, 'dropout': 0.1980138964759086, 'lr': 0.004244603486731906}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:31,631] Trial 8 finished with value: 1.0 and parameters: {'n_layers': 2, 'units': 32, 'dropout': 0.32830532293939974, 'lr': 0.008134483284823378}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:36,749] Trial 9 finished with value: 1.0 and parameters: {'n_layers': 3, 'units': 112, 'dropout': 0.42787751190641105, 'lr': 0.007857568722841797}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:41,872] Trial 10 finished with value: 0.5555555820465088 and parameters: {'n_layers': 3, 'units': 16, 'dropout': 0.0016786346342751168, 'lr': 0.00010331661413660463}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:46,567] Trial 11 finished with value: 1.0 and parameters: {'n_layers': 2, 'units': 64, 'dropout': 0.0757042403480773, 'lr': 0.0017176258904247213}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:51,404] Trial 12 finished with value: 0.9722222089767456 and parameters: {'n_layers': 2, 'units': 16, 'dropout': 0.13039773212828237, 'lr': 0.000665340348539231}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:09:55,340] Trial 13 finished with value: 1.0 and parameters: {'n_layers': 1, 'units': 48, 'dropout': 0.0045602599791011444, 'lr': 0.0024848502020764795}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:10:00,342] Trial 14 finished with value: 1.0 and parameters: {'n_layers': 2, 'units': 128, 'dropout': 0.1606946908118007, 'lr': 0.00040207502350634504}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:10:04,897] Trial 15 finished with value: 0.9722222089767456 and parameters: {'n_layers': 2, 'units': 80, 'dropout': 0.06471897197568668, 'lr': 0.0001929686463903042}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:10:09,998] Trial 16 finished with value: 1.0 and parameters: {'n_layers': 3, 'units': 48, 'dropout': 0.37604365147131935, 'lr': 0.0006502961026634716}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:10:13,913] Trial 17 finished with value: 1.0 and parameters: {'n_layers': 1, 'units': 32, 'dropout': 0.25042627151546715, 'lr': 0.005380743365468689}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:10:18,461] Trial 18 finished with value: 1.0 and parameters: {'n_layers': 2, 'units': 96, 'dropout': 0.07168493779036797, 'lr': 0.001958556585240843}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-09-10 10:10:23,561] Trial 19 finished with value: 1.0 and parameters: {'n_layers': 3, 'units': 32, 'dropout': 0.18450319111810815, 'lr': 0.00999897834239622}. Best is trial 0 with value: 1.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'n_layers': 3, 'units': 16, 'dropout': 0.07802871566412534, 'lr': 0.007825365905628434}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "\n",
    "# === 1. Dataset ===\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === 2. Definir función objetivo ===\n",
    "def objective(trial):\n",
    "    # Hiperparámetros a explorar\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    units = trial.suggest_int(\"units\", 16, 128, step=16)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # Construcción del modelo\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(X_train.shape[1],)))\n",
    "    for _ in range(n_layers):\n",
    "        model.add(keras.layers.Dense(units, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(dropout))\n",
    "    model.add(keras.layers.Dense(len(set(y)), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Entrenamiento rápido\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Mejor accuracy en validación\n",
    "    val_acc = max(history.history[\"val_accuracy\"])\n",
    "    return val_acc\n",
    "\n",
    "# === 3. Ejecutar optimización ===\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f941a5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mejor accuracy:\", study.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a5715",
   "metadata": {},
   "source": [
    "# Persisting your models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55e5c1",
   "metadata": {},
   "source": [
    "En el capítulo anterior, presentamos algunos de los conceptos básicos del control de versiones de modelos utilizando MLflow. En particular, discutimos cómo registrar métricas para tus experimentos de ML utilizando la API de seguimiento de MLflow. Ahora vamos a construir sobre este conocimiento y considerar los puntos de contacto que nuestros sistemas de entrenamiento deben tener con los sistemas de control de modelos en general.\n",
    "\n",
    "Primero, hagamos un resumen de lo que estamos tratando de hacer con el sistema de entrenamiento. Queremos automatizar (en la medida de lo posible) gran parte del trabajo que realizaron los científicos de datos al encontrar el primer modelo funcional, para que podamos actualizar y crear continuamente nuevas versiones del modelo que aún resuelvan el problema en el futuro. También nos gustaría tener un mecanismo simple que permita compartir los resultados del proceso de entrenamiento con la parte de la solución que llevará a cabo la predicción cuando esté en producción. Podemos pensar en nuestro sistema de control de versiones del modelo como un puente entre las diferentes etapas del proceso de desarrollo de ML que discutimos en el Capítulo 2, El Proceso de Desarrollo de Aprendizaje Automático.\n",
    "\n",
    "En particular, podemos ver que la capacidad de rastrear los resultados de los experimentos nos permite mantener los resultados de la fase de Prueba y construir sobre ellos durante la fase de Desarrollo. También podemos rastrear más experimentos, ejecuciones de pruebas y resultados de optimización de hiperparámetros en el mismo lugar durante la fase de Desarrollo. Luego, podemos comenzar a etiquetar los modelos eficientes como aquellos que son buenos candidatos para el despliegue, cerrando así la brecha entre las fases de desarrollo de Desarrollo y Despliegue. Si nos enfocamos en MLflow por ahora (aunque hay muchas otras soluciones disponibles que cumplen con la necesidad de un sistema de control de versiones de modelos), las funcionalidades de Seguimiento y Registro de Modelos de MLflow se integran muy bien en estos roles de puente. Esto se representa esquemáticamente en el siguiente diagrama:\n",
    "\n",
    "![Ml-flow](figures/ML-flow.png)\n",
    "\n",
    "En el Capítulo 2, El Proceso de Desarrollo de Aprendizaje Automático, solo exploramos lo básico de la API de Seguimiento de MLflow para almacenar los metadatos de ejecución de modelos experimentales. Ahora, haremos una breve inmersión en cómo almacenar modelos listos para producción de manera muy organizada para que puedas comenzar a realizar la preparación de modelos. Este es el proceso mediante el cual los modelos pueden avanzar a través de etapas de preparación, y puedes intercambiar modelos en producción si lo deseas. Esta es una parte extremadamente importante de cualquier sistema de entrenamiento que proporciona modelos y que funcionará como parte de una solución implementada, ¡que es de lo que trata este libro!\n",
    "\n",
    "Como se mencionó anteriormente, la funcionalidad que necesitamos en MLflow se llama Registro de Modelos, una de las cuatro funcionalidades principales de MLflow. Aquí, pasaremos por ejemplos de cómo tomar un modelo registrado y enviarlo al registro, cómo actualizar información como el número de versión del modelo en el registro, y luego cómo avanzar su modelo a través de diferentes etapas del ciclo de vida. Terminaremos esta sección aprendiendo cómo recuperar un modelo dado del registro en otros programas, un punto clave si queremos compartir nuestros modelos entre servicios de entrenamiento y predicción separados.\n",
    "\n",
    "Antes de sumergirnos en el código de Python para interactuar con el Registro de Modelos, tenemos una pieza importante de configuración que realizar. El registro solo funciona si se está utilizando una base de datos para almacenar los metadatos y parámetros del modelo. Esto es diferente de la API de Seguimiento básica, que funciona solo con un almacén de archivos. Esto significa que antes de enviar modelos al Registro de Modelos, debemos iniciar un servidor de MLflow con un backend de base de datos. Puedes hacer esto con una base de datos PostgreSQL ejecutándose localmente al ejecutar el siguiente comando en tu terminal. Tendrás que ejecutar esto antes de los fragmentos de código en el resto de esta sección.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662bb51f",
   "metadata": {},
   "source": [
    "Ahora que la base de datos del backend está en funcionamiento, podemos usarla como parte de nuestro flujo de trabajo del modelo. Empecemos:\n",
    "\n",
    "1. Comencemos registrando algunas métricas y parámetros para uno de los modelos que entrenamos anteriormente en este capítulo:\n",
    "\n",
    "input_example = pd.DataFrame(X_train[:1], columns=wine.feature_names)\n",
    "\n",
    "¿Por qué se hace esto?\n",
    "\n",
    "Ese input_example se pasa al registrar el modelo en MLflow.\n",
    "\n",
    "MLflow lo usa para inferir la firma del modelo automáticamente:\n",
    "\n",
    "- número de columnas,\n",
    "\n",
    "- nombres de features,\n",
    "\n",
    "- tipos de datos (float, int, etc.).\n",
    "\n",
    "Con eso, cuando vayas a servir el modelo (por ejemplo, vía API), MLflow sabrá qué tipo de datos espera y cómo validarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042fc849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luisgarcia/anaconda3/envs/Tensorflow_GPU/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Registered model 'sk-learn-std-scale-clf' already exists. Creating a new version of this model...\n",
      "2025/09/10 14:50:09 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: sk-learn-std-scale-clf, version 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run YOUR_RUN_NAME at: http://0.0.0.0:5000/#/experiments/0/runs/d57587359d974d4081983c51c23c79d5\n",
      "🧪 View experiment at: http://0.0.0.0:5000/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '3' of model 'sk-learn-std-scale-clf'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Conectar el script al servidor MLflow\n",
    "mlflow.set_tracking_uri(\"http://0.0.0.0:5000\")\n",
    "# === 1. Dataset ===\n",
    "wine = load_wine()\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear un input_example (para que MLflow infiera la firma)\n",
    "input_example = pd.DataFrame(X_train[:1], columns=wine.feature_names)\n",
    "\n",
    "with mlflow.start_run(run_name=\"YOUR_RUN_NAME\") as run:\n",
    "    params = {'tol': 1e-2,'solver': 'sag'}\n",
    "    std_scale_clf = make_pipeline(StandardScaler(), RidgeClassifier(**params))\n",
    "\n",
    "    std_scale_clf.fit(X_train, y_train)\n",
    "    y_pred_std_scale = std_scale_clf.predict(X_test)\n",
    "\n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            'accuracy': metrics.accuracy_score(y_test, y_pred_std_scale),\n",
    "            'precision': metrics.precision_score(y_test, y_pred_std_scale,average='macro'),\n",
    "            'f1': metrics.f1_score(y_test, y_pred_std_scale, average='macro'),\n",
    "            'recall': metrics.recall_score(y_test, y_pred_std_scale,average='macro')\n",
    "        }\n",
    "    )\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    \"\"\"Dentro del mismo bloque de código, ahora podemos registrar el modelo en el Registro de Modelos, \n",
    "    proporcionando un nombre para hacer referencia al modelo más tarde:\n",
    "    \"\"\"\n",
    "    # Registrar el modelo en el Model Registry con input_example\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=std_scale_clf,\n",
    "        name=\"sk-learn-std-scale-clf\",   # reemplaza artifact_path\n",
    "        registered_model_name=\"sk-learn-std-scale-clf\",\n",
    "        input_example=input_example\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31e672",
   "metadata": {},
   "source": [
    "Ahora, supongamos que estamos ejecutando un servicio de predicción y queremos recuperar el modelo y predecir utilizando este. Aquí, tenemos que escribir lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd493d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 0, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 2, 2, 2, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"sk-learn-std-scale-clf\"\n",
    "model_version = 3\n",
    "model = mlflow.pyfunc.load_model(\n",
    "    model_uri=f\"models:/{model_name}/{model_version}\"\n",
    ")\n",
    "X_test = pd.DataFrame(X_test, columns=wine.feature_names)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2a2cd",
   "metadata": {},
   "source": [
    "Por defecto, los modelos recién registrados en el Registro de Modelos se asignan al valor de etapa 'None'. Por lo tanto, debemos agregarle un alias de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec9adbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "client.set_registered_model_alias(\n",
    "    name=\"sk-learn-std-scale-clf\",\n",
    "    alias=\"staging\",\n",
    "    version=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5118f28",
   "metadata": {},
   "source": [
    "Basado en todas nuestras conversaciones en este capítulo, el resultado de nuestro sistema de entrenamiento debe ser capaz de producir un modelo que estemos satisfechos de implementar en producción. La siguiente pieza de código promociona el modelo a una etapa diferente, llamada \"Producción\":."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b3fcbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar el alias \"production\" al modelo versión 1\n",
    "client.set_registered_model_alias(\n",
    "    name=\"sk-learn-std-scale-clf\",\n",
    "    alias=\"production\",\n",
    "    version=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85760c9b",
   "metadata": {},
   "source": [
    "Estas son las formas más importantes de interactuar con el Registro de Modelos y hemos cubierto lo básico sobre cómo registrar, actualizar, promover y recuperar tus modelos en tus sistemas de entrenamiento (y predicción).\n",
    "\n",
    "Ahora, aprenderemos cómo encadenar nuestros principales pasos de entrenamiento en unidades singulares llamadas pipelines. Cubriremos algunas de las formas estándar de hacer esto dentro de scripts únicos, lo que nos permitirá construir nuestros primeros pipelines de entrenamiento. En el Capítulo 5, Patrones y Herramientas de Despliegue, cubriremos herramientas para construir pipelines de software más genéricos para tu solución de ML (de los cuales tu pipeline de entrenamiento puede ser un único componente)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e1883",
   "metadata": {},
   "source": [
    "# Construyendo la fabrica de modelos con pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc406b4",
   "metadata": {},
   "source": [
    "El concepto de un pipeline de software es lo suficientemente intuitivo. Si tienes una serie de pasos encadenados en tu código, de manera que el siguiente paso consume o utiliza la salida del paso o pasos anteriores, entonces tienes un pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f14a1",
   "metadata": {},
   "source": [
    "## Scikit-learn pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa719d1",
   "metadata": {},
   "source": [
    "Nuestro viejo amigo scikit-learn viene empaquetado con una agradable funcionalidad de creación de pipelines. En el momento de escribir esto, las versiones de scikit-learn superiores a 0.20 también contienen el objeto ColumnTransformer, que te permite construir pipelines que realizan diferentes acciones en columnas específicas. Esto es exactamente lo que queremos hacer con el ejemplo del modelo de marketing de regresión logística que discutíamos anteriormente, donde queremos estandarizar nuestros valores numéricos y codificar en dummys nuestras variables categóricas. Comencemos:\n",
    "\n",
    "1. Para crear esta pipeline, necesitas importar los objetos ColumnTransformer y Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = lambda file: os.path.join(os.getcwd(),'data',file)\n",
    "bank_df = pd.read_csv(file_path('bank.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115647f5",
   "metadata": {},
   "source": [
    "2. Ahora, debemos definir el sub-pipeline del transformador numérico, que contiene los dos pasos para la imputación y la escalación. También debemos definir los nombres de las columnas numéricas a las que se aplicará esto para que podamos usarlas más tarde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5163d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'balance']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a1a62",
   "metadata": {},
   "source": [
    "3. A continuación, debemos realizar pasos similares para las variables categóricas, pero aquí, solo tenemos un paso de transformación que definir para el codificador one-hot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2ff6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "categorical_features = ['job', 'marital', 'education', 'contact', 'housing', 'loan','default','day']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b0823",
   "metadata": {},
   "source": [
    "5. Debemos reunir todos estos pasos de preprocesamiento en un solo objeto, llamado preprocesador, utilizando el objeto ColumnTransformer. Esto aplicará nuestros transformadores a las columnas apropiadas de nuestro DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86669882",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7f39e",
   "metadata": {},
   "source": [
    "6. Finalmente, queremos agregar el paso del modelo de ML al final de los pasos anteriores y finalizar la pipéline. Llamaremos a esto clf_pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cecafcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3843e50",
   "metadata": {},
   "source": [
    "Este es nuestro primer pipeline de entrenamiento de ML. La belleza de la API de scikit-learn es que el objeto clf_pipeline ahora puede ser llamado como si fuera un algoritmo estándar del resto de la biblioteca. Así que esto significa que podemos escribir lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "794915cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
       "                                                                  (&#x27;scaler&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;age&#x27;, &#x27;balance&#x27;]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;job&#x27;, &#x27;marital&#x27;,\n",
       "                                                   &#x27;education&#x27;, &#x27;contact&#x27;,\n",
       "                                                   &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;default&#x27;,\n",
       "                                                   &#x27;day&#x27;])])),\n",
       "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>Pipeline</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('steps',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">steps&nbsp;</td>\n",
       "            <td class=\"value\">[(&#x27;preprocessor&#x27;, ...), (&#x27;classifier&#x27;, ...)]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transform_input',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transform_input&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('memory',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">memory&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>preprocessor: ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for preprocessor: ColumnTransformer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"preprocessor__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformers',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transformers&nbsp;</td>\n",
       "            <td class=\"value\">[(&#x27;num&#x27;, ...), (&#x27;cat&#x27;, ...)]</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('remainder',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">remainder&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;drop&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_threshold',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sparse_threshold&nbsp;</td>\n",
       "            <td class=\"value\">0.3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('transformer_weights',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">transformer_weights&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose_feature_names_out',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose_feature_names_out&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('force_int_remainder_cols',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">force_int_remainder_cols&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>num</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"preprocessor__num__\"><pre>[&#x27;age&#x27;, &#x27;balance&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SimpleImputer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"preprocessor__num__imputer__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('missing_values',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">missing_values&nbsp;</td>\n",
       "            <td class=\"value\">nan</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('strategy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">strategy&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;median&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fill_value',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fill_value&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('add_indicator',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">add_indicator&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('keep_empty_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">keep_empty_features&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>StandardScaler</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"preprocessor__num__scaler__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_mean',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_mean&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('with_std',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">with_std&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>cat</div></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"preprocessor__cat__\"><pre>[&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;contact&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;default&#x27;, &#x27;day&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"preprocessor__cat__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">categories&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;auto&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('drop',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">drop&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('sparse_output',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">sparse_output&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dtype',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dtype&nbsp;</td>\n",
       "            <td class=\"value\">&lt;class &#x27;numpy.float64&#x27;&gt;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('handle_unknown',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">handle_unknown&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;ignore&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_frequency',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_frequency&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_categories',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_categories&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('feature_name_combiner',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">feature_name_combiner&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;concat&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"classifier__\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['age', 'balance']),\n",
       "                                                 ('cat',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['job', 'marital',\n",
       "                                                   'education', 'contact',\n",
       "                                                   'housing', 'loan', 'default',\n",
       "                                                   'day'])])),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = bank_df.drop(columns=['deposit'],axis=1)\n",
    "y = bank_df['deposit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af906a",
   "metadata": {},
   "source": [
    "Esto ejecutará los métodos de ajuste de todos los pasos del pipeline en secuencia. La capacidad de abstraer los pasos que realizan la ingeniería de características y entrenan tu modelo en un solo objeto es muy poderosa, ya que significa que puedes exportar e importar este pipeline en varios lugares, sin conocer los detalles de la implementación. ¡La abstracción es algo bueno!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
